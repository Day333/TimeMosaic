Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ETTm1_96_96         Model:              AGPTp               

[1mData Loader[0m
  Data:               ETTm1               Root Path:          ./dataset/ETT-small/
  Data Path:          ETTm1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            512                 
  n heads:            2                   e layers:           1                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ETTm1_96_96_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34369
val 11425
test 11425
	iters: 100, epoch: 1 | loss: 0.4137251
	speed: 0.0874s/iter; left time: 931.0514s
	iters: 200, epoch: 1 | loss: 0.4115053
	speed: 0.0820s/iter; left time: 865.6655s
	iters: 300, epoch: 1 | loss: 0.3949969
	speed: 0.0819s/iter; left time: 855.7463s
	iters: 400, epoch: 1 | loss: 0.3070329
	speed: 0.0817s/iter; left time: 845.3379s
	iters: 500, epoch: 1 | loss: 0.3013275
	speed: 0.0819s/iter; left time: 839.3246s
	iters: 600, epoch: 1 | loss: 0.2938636
	speed: 0.0817s/iter; left time: 829.6710s
	iters: 700, epoch: 1 | loss: 0.3216852
	speed: 0.0815s/iter; left time: 819.6080s
	iters: 800, epoch: 1 | loss: 0.3274186
	speed: 0.0817s/iter; left time: 812.8313s
	iters: 900, epoch: 1 | loss: 0.3255163
	speed: 0.0818s/iter; left time: 805.7248s
	iters: 1000, epoch: 1 | loss: 0.3203992
	speed: 0.0819s/iter; left time: 798.1735s
Epoch: 1 cost time: 88.5174150466919
Epoch: 1, Steps: 1075 | Train Loss: 0.3455147 Vali Loss: 0.4159215 Test Loss: 0.3367646
Validation loss decreased (inf --> 0.415922).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3651729
	speed: 0.4477s/iter; left time: 4287.5937s
	iters: 200, epoch: 2 | loss: 0.2585096
	speed: 0.0817s/iter; left time: 773.7588s
	iters: 300, epoch: 2 | loss: 0.3041135
	speed: 0.0820s/iter; left time: 768.4123s
	iters: 400, epoch: 2 | loss: 0.3697234
	speed: 0.0820s/iter; left time: 760.2729s
	iters: 500, epoch: 2 | loss: 0.2597628
	speed: 0.0820s/iter; left time: 752.2480s
	iters: 600, epoch: 2 | loss: 0.3265950
	speed: 0.0821s/iter; left time: 744.7045s
	iters: 700, epoch: 2 | loss: 0.3254799
	speed: 0.0817s/iter; left time: 733.5573s
	iters: 800, epoch: 2 | loss: 0.3138115
	speed: 0.0819s/iter; left time: 726.8966s
	iters: 900, epoch: 2 | loss: 0.2771727
	speed: 0.0818s/iter; left time: 718.2573s
	iters: 1000, epoch: 2 | loss: 0.3151079
	speed: 0.0818s/iter; left time: 709.3523s
Epoch: 2 cost time: 88.17372703552246
Epoch: 2, Steps: 1075 | Train Loss: 0.3202369 Vali Loss: 0.4231760 Test Loss: 0.3386363
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.3176099
	speed: 0.4475s/iter; left time: 3803.8750s
	iters: 200, epoch: 3 | loss: 0.3074861
	speed: 0.0821s/iter; left time: 689.4418s
	iters: 300, epoch: 3 | loss: 0.2989990
	speed: 0.0817s/iter; left time: 677.9984s
	iters: 400, epoch: 3 | loss: 0.3379770
	speed: 0.0817s/iter; left time: 669.7876s
	iters: 500, epoch: 3 | loss: 0.2779777
	speed: 0.0819s/iter; left time: 663.7636s
	iters: 600, epoch: 3 | loss: 0.3381512
	speed: 0.0815s/iter; left time: 651.8616s
	iters: 700, epoch: 3 | loss: 0.2840088
	speed: 0.0817s/iter; left time: 645.3280s
	iters: 800, epoch: 3 | loss: 0.2683802
	speed: 0.0819s/iter; left time: 638.8679s
	iters: 900, epoch: 3 | loss: 0.2935852
	speed: 0.0817s/iter; left time: 629.3656s
	iters: 1000, epoch: 3 | loss: 0.2149637
	speed: 0.0821s/iter; left time: 624.1626s
Epoch: 3 cost time: 88.1297619342804
Epoch: 3, Steps: 1075 | Train Loss: 0.3068305 Vali Loss: 0.4049281 Test Loss: 0.3337532
Validation loss decreased (0.415922 --> 0.404928).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.3097409
	speed: 0.4234s/iter; left time: 3144.4944s
	iters: 200, epoch: 4 | loss: 0.2687972
	speed: 0.0732s/iter; left time: 536.2698s
	iters: 300, epoch: 4 | loss: 0.2800695
	speed: 0.0737s/iter; left time: 532.2868s
	iters: 400, epoch: 4 | loss: 0.2954102
	speed: 0.0734s/iter; left time: 523.0543s
	iters: 500, epoch: 4 | loss: 0.3296769
	speed: 0.0734s/iter; left time: 515.3725s
	iters: 600, epoch: 4 | loss: 0.2506338
	speed: 0.0719s/iter; left time: 497.7416s
	iters: 700, epoch: 4 | loss: 0.3352208
	speed: 0.0748s/iter; left time: 510.8126s
	iters: 800, epoch: 4 | loss: 0.2991707
	speed: 0.0682s/iter; left time: 458.8518s
	iters: 900, epoch: 4 | loss: 0.3333220
	speed: 0.0737s/iter; left time: 488.0368s
	iters: 1000, epoch: 4 | loss: 0.2848112
	speed: 0.0731s/iter; left time: 476.8111s
Epoch: 4 cost time: 78.62967896461487
Epoch: 4, Steps: 1075 | Train Loss: 0.3010726 Vali Loss: 0.4127491 Test Loss: 0.3238096
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.3534733
	speed: 0.4003s/iter; left time: 2542.2778s
	iters: 200, epoch: 5 | loss: 0.3102091
	speed: 0.0769s/iter; left time: 480.7951s
	iters: 300, epoch: 5 | loss: 0.2437761
	speed: 0.0767s/iter; left time: 471.4878s
	iters: 400, epoch: 5 | loss: 0.3271747
	speed: 0.0754s/iter; left time: 456.4660s
	iters: 500, epoch: 5 | loss: 0.3980201
	speed: 0.0767s/iter; left time: 456.4871s
	iters: 600, epoch: 5 | loss: 0.3157122
	speed: 0.0746s/iter; left time: 436.6945s
	iters: 700, epoch: 5 | loss: 0.3411957
	speed: 0.0759s/iter; left time: 436.3724s
	iters: 800, epoch: 5 | loss: 0.3038983
	speed: 0.0765s/iter; left time: 432.4266s
	iters: 900, epoch: 5 | loss: 0.2920813
	speed: 0.0768s/iter; left time: 426.1122s
	iters: 1000, epoch: 5 | loss: 0.3250411
	speed: 0.0755s/iter; left time: 411.3755s
Epoch: 5 cost time: 81.72857761383057
Epoch: 5, Steps: 1075 | Train Loss: 0.2979573 Vali Loss: 0.4080285 Test Loss: 0.3247416
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.3076226
	speed: 0.4300s/iter; left time: 2268.8164s
	iters: 200, epoch: 6 | loss: 0.2533545
	speed: 0.0817s/iter; left time: 422.8420s
	iters: 300, epoch: 6 | loss: 0.2942529
	speed: 0.0816s/iter; left time: 413.9534s
	iters: 400, epoch: 6 | loss: 0.2744537
	speed: 0.0817s/iter; left time: 406.3582s
	iters: 500, epoch: 6 | loss: 0.3224457
	speed: 0.0817s/iter; left time: 398.1480s
	iters: 600, epoch: 6 | loss: 0.3029997
	speed: 0.0820s/iter; left time: 391.8517s
	iters: 700, epoch: 6 | loss: 0.2979326
	speed: 0.0816s/iter; left time: 381.7233s
	iters: 800, epoch: 6 | loss: 0.2942357
	speed: 0.0812s/iter; left time: 371.6764s
	iters: 900, epoch: 6 | loss: 0.2587498
	speed: 0.0817s/iter; left time: 365.4698s
	iters: 1000, epoch: 6 | loss: 0.2621900
	speed: 0.0817s/iter; left time: 357.3011s
Epoch: 6 cost time: 87.93197107315063
Epoch: 6, Steps: 1075 | Train Loss: 0.2968327 Vali Loss: 0.4113043 Test Loss: 0.3221645
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_ETTm1_96_96_AGPTp_2048_fixedFalse_0.0001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11425
test shape: (11425, 96, 7) (11425, 96, 7)
test shape: (11425, 96, 7) (11425, 96, 7)
mse:0.3345101475715637, mae:0.36894652247428894
================================================================================
Model Profiling Summary
Total Params             : 3,830,115
Inference Time (s)       : 0.032278
GPU Mem Footprint (MB)   : 55.78
Peak Mem (MB)            : 149.97
================================================================================
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ETTm1_96_192        Model:              AGPTp               

[1mData Loader[0m
  Data:               ETTm1               Root Path:          ./dataset/ETT-small/
  Data Path:          ETTm1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            512                 
  n heads:            2                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         128                 
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ETTm1_96_192_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34273
val 11329
test 11329
	iters: 100, epoch: 1 | loss: 0.3588184
	speed: 1.3296s/iter; left time: 3431.6756s
	iters: 200, epoch: 1 | loss: 0.3798492
	speed: 1.3324s/iter; left time: 3305.7634s
Epoch: 1 cost time: 357.0734398365021
Epoch: 1, Steps: 268 | Train Loss: 0.3823135 Vali Loss: 0.5361143 Test Loss: 0.3812294
Validation loss decreased (inf --> 0.536114).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3452464
	speed: 2.8167s/iter; left time: 6514.9731s
	iters: 200, epoch: 2 | loss: 0.3398156
	speed: 1.3332s/iter; left time: 2950.2620s
Epoch: 2 cost time: 356.843624830246
Epoch: 2, Steps: 268 | Train Loss: 0.3604001 Vali Loss: 0.5303719 Test Loss: 0.3707358
Validation loss decreased (0.536114 --> 0.530372).  Saving model ...
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.3725180
	speed: 2.8261s/iter; left time: 5779.4125s
	iters: 200, epoch: 3 | loss: 0.3497552
	speed: 1.3333s/iter; left time: 2593.1862s
Epoch: 3 cost time: 356.840341091156
Epoch: 3, Steps: 268 | Train Loss: 0.3498906 Vali Loss: 0.5256193 Test Loss: 0.3717888
Validation loss decreased (0.530372 --> 0.525619).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.3493195
	speed: 2.8280s/iter; left time: 5025.4034s
	iters: 200, epoch: 4 | loss: 0.3587010
	speed: 1.3351s/iter; left time: 2239.0464s
Epoch: 4 cost time: 357.3685722351074
Epoch: 4, Steps: 268 | Train Loss: 0.3448301 Vali Loss: 0.5291918 Test Loss: 0.3669405
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.3399776
	speed: 2.8355s/iter; left time: 4278.8435s
	iters: 200, epoch: 5 | loss: 0.3145704
	speed: 1.3369s/iter; left time: 1883.7157s
Epoch: 5 cost time: 357.9927158355713
Epoch: 5, Steps: 268 | Train Loss: 0.3420618 Vali Loss: 0.5259567 Test Loss: 0.3693931
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.3771223
	speed: 2.8317s/iter; left time: 3514.0822s
	iters: 200, epoch: 6 | loss: 0.3259380
	speed: 1.3350s/iter; left time: 1523.2233s
Epoch: 6 cost time: 357.45458006858826
Epoch: 6, Steps: 268 | Train Loss: 0.3407833 Vali Loss: 0.5267353 Test Loss: 0.3677637
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_ETTm1_96_192_AGPTp_2048_fixedFalse_0.0001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11329
test shape: (11329, 192, 7) (11329, 192, 7)
test shape: (11329, 192, 7) (11329, 192, 7)
mse:0.3724249601364136, mae:0.3897252678871155
================================================================================
Model Profiling Summary
Total Params             : 10,730,947
Inference Time (s)       : 0.298965
GPU Mem Footprint (MB)   : 112.09
Peak Mem (MB)            : 503.12
================================================================================
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ETTm1_96_336        Model:              AGPTp               

[1mData Loader[0m
  Data:               ETTm1               Root Path:          ./dataset/ETT-small/
  Data Path:          ETTm1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            512                 
  n heads:            4                   e layers:           1                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         128                 
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ETTm1_96_336_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 34129
val 11185
test 11185
	iters: 100, epoch: 1 | loss: 0.4497422
	speed: 0.5375s/iter; left time: 1381.8393s
	iters: 200, epoch: 1 | loss: 0.4034578
	speed: 0.5282s/iter; left time: 1305.2245s
Epoch: 1 cost time: 141.66606259346008
Epoch: 1, Steps: 267 | Train Loss: 0.4399386 Vali Loss: 0.6817133 Test Loss: 0.4153621
Validation loss decreased (inf --> 0.681713).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.3853828
	speed: 1.1600s/iter; left time: 2672.6747s
	iters: 200, epoch: 2 | loss: 0.4097114
	speed: 0.5456s/iter; left time: 1202.5254s
Epoch: 2 cost time: 144.93096375465393
Epoch: 2, Steps: 267 | Train Loss: 0.4174637 Vali Loss: 0.6824245 Test Loss: 0.4091091
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.4211665
	speed: 1.2110s/iter; left time: 2466.8708s
	iters: 200, epoch: 3 | loss: 0.4138321
	speed: 0.5570s/iter; left time: 1078.8326s
Epoch: 3 cost time: 148.47651553153992
Epoch: 3, Steps: 267 | Train Loss: 0.4097219 Vali Loss: 0.6747137 Test Loss: 0.4030097
Validation loss decreased (0.681713 --> 0.674714).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.3612964
	speed: 1.2189s/iter; left time: 2157.4492s
	iters: 200, epoch: 4 | loss: 0.3951363
	speed: 0.5565s/iter; left time: 929.3533s
Epoch: 4 cost time: 148.4490361213684
Epoch: 4, Steps: 267 | Train Loss: 0.4059020 Vali Loss: 0.6773610 Test Loss: 0.4046244
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.4159140
	speed: 1.2192s/iter; left time: 1832.4537s
	iters: 200, epoch: 5 | loss: 0.4040223
	speed: 0.5560s/iter; left time: 780.1287s
Epoch: 5 cost time: 148.4959011077881
Epoch: 5, Steps: 267 | Train Loss: 0.4040604 Vali Loss: 0.6729696 Test Loss: 0.4025690
Validation loss decreased (0.674714 --> 0.672970).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.3844826
	speed: 1.2217s/iter; left time: 1509.9692s
	iters: 200, epoch: 6 | loss: 0.3699336
	speed: 0.5556s/iter; left time: 631.1654s
Epoch: 6 cost time: 148.4674551486969
Epoch: 6, Steps: 267 | Train Loss: 0.4030299 Vali Loss: 0.6745971 Test Loss: 0.4019095
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.4168957
	speed: 1.2182s/iter; left time: 1180.4107s
	iters: 200, epoch: 7 | loss: 0.3821503
	speed: 0.5562s/iter; left time: 483.3738s
Epoch: 7 cost time: 148.37453770637512
Epoch: 7, Steps: 267 | Train Loss: 0.4026908 Vali Loss: 0.6712071 Test Loss: 0.4015822
Validation loss decreased (0.672970 --> 0.671207).  Saving model ...
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.3874462
	speed: 1.2182s/iter; left time: 855.1890s
	iters: 200, epoch: 8 | loss: 0.4172161
	speed: 0.5564s/iter; left time: 334.9340s
Epoch: 8 cost time: 148.33378911018372
Epoch: 8, Steps: 267 | Train Loss: 0.4025430 Vali Loss: 0.6740628 Test Loss: 0.4016577
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.3958952
	speed: 1.2178s/iter; left time: 529.7300s
	iters: 200, epoch: 9 | loss: 0.3875926
	speed: 0.5561s/iter; left time: 186.2792s
Epoch: 9 cost time: 148.28498315811157
Epoch: 9, Steps: 267 | Train Loss: 0.4023124 Vali Loss: 0.6712542 Test Loss: 0.4017969
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
	iters: 100, epoch: 10 | loss: 0.3831175
	speed: 1.2197s/iter; left time: 204.9119s
	iters: 200, epoch: 10 | loss: 0.3542019
	speed: 0.5563s/iter; left time: 37.8283s
Epoch: 10 cost time: 148.52094984054565
Epoch: 10, Steps: 267 | Train Loss: 0.4022927 Vali Loss: 0.6727610 Test Loss: 0.4024391
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_ETTm1_96_336_AGPTp_2048_fixedFalse_0.0001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 11185
test shape: (11185, 336, 7) (11185, 336, 7)
test shape: (11185, 336, 7) (11185, 336, 7)
mse:0.4012128412723541, mae:0.40774643421173096
================================================================================
Model Profiling Summary
Total Params             : 5,313,107
Inference Time (s)       : 0.127385
GPU Mem Footprint (MB)   : 72.59
Peak Mem (MB)            : 437.34
================================================================================
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ETTm1_96_720        Model:              AGPTp               

[1mData Loader[0m
  Data:               ETTm1               Root Path:          ./dataset/ETT-small/
  Data Path:          ETTm1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            512                 
  n heads:            4                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         128                 
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ETTm1_96_720_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 33745
val 10801
test 10801
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 194, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 344, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 275, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 106.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 34.12 MiB is free. Process 1259810 has 36.17 GiB memory in use. Process 2723360 has 42.94 GiB memory in use. Of the allocated memory 42.32 GiB is allocated by PyTorch, and 132.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
