Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_96           Model:              AGPTp               

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_96_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 18221
val 2537
test 5165
	iters: 100, epoch: 1 | loss: 0.2903271
	speed: 3.6561s/iter; left time: 41280.5159s
	iters: 200, epoch: 1 | loss: 0.2780381
	speed: 3.6134s/iter; left time: 40437.6685s
	iters: 300, epoch: 1 | loss: 0.2754116
	speed: 3.6767s/iter; left time: 40777.9524s
	iters: 400, epoch: 1 | loss: 0.2972430
	speed: 3.6770s/iter; left time: 40414.1140s
	iters: 500, epoch: 1 | loss: 0.2576621
	speed: 3.6171s/iter; left time: 39393.4515s
	iters: 600, epoch: 1 | loss: 0.2573299
	speed: 3.6774s/iter; left time: 39683.1180s
	iters: 700, epoch: 1 | loss: 0.2810158
	speed: 3.6502s/iter; left time: 39024.2518s
	iters: 800, epoch: 1 | loss: 0.2368291
	speed: 3.6472s/iter; left time: 38627.7339s
	iters: 900, epoch: 1 | loss: 0.2672109
	speed: 3.6766s/iter; left time: 38571.5453s
	iters: 1000, epoch: 1 | loss: 0.2669228
	speed: 3.6095s/iter; left time: 37506.4962s
	iters: 1100, epoch: 1 | loss: 0.2847316
	speed: 3.5422s/iter; left time: 36452.6387s
Epoch: 1 cost time: 4141.639783620834
Epoch: 1, Steps: 1139 | Train Loss: 0.2845324 Vali Loss: 0.1723741 Test Loss: 0.1950275
Validation loss decreased (inf --> 0.172374).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2723579
	speed: 8.9525s/iter; left time: 90886.0468s
	iters: 200, epoch: 2 | loss: 0.2508300
	speed: 3.6518s/iter; left time: 36707.7451s
	iters: 300, epoch: 2 | loss: 0.2769425
	speed: 3.6462s/iter; left time: 36286.5798s
	iters: 400, epoch: 2 | loss: 0.2916026
	speed: 3.6308s/iter; left time: 35770.7699s
	iters: 500, epoch: 2 | loss: 0.2531412
	speed: 3.6472s/iter; left time: 35567.4756s
	iters: 600, epoch: 2 | loss: 0.2448927
	speed: 3.6540s/iter; left time: 35268.1940s
	iters: 700, epoch: 2 | loss: 0.2534389
	speed: 3.6312s/iter; left time: 34684.8978s
	iters: 800, epoch: 2 | loss: 0.2561986
	speed: 3.6391s/iter; left time: 34396.7527s
	iters: 900, epoch: 2 | loss: 0.2616090
	speed: 3.6573s/iter; left time: 34203.0810s
	iters: 1000, epoch: 2 | loss: 0.2551846
	speed: 3.6310s/iter; left time: 33594.0615s
	iters: 1100, epoch: 2 | loss: 0.2589238
	speed: 3.6344s/iter; left time: 33262.0818s
Epoch: 2 cost time: 4145.563459396362
Epoch: 2, Steps: 1139 | Train Loss: 0.2617990 Vali Loss: 0.1733325 Test Loss: 0.1949963
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2244420
	speed: 9.0331s/iter; left time: 81415.0527s
	iters: 200, epoch: 3 | loss: 0.2736953
	speed: 3.4204s/iter; left time: 30486.1845s
	iters: 300, epoch: 3 | loss: 0.2534456
	speed: 3.4899s/iter; left time: 30756.5969s
	iters: 400, epoch: 3 | loss: 0.2816603
	speed: 3.5744s/iter; left time: 31143.8206s
	iters: 500, epoch: 3 | loss: 0.2360699
	speed: 3.5797s/iter; left time: 30832.1813s
	iters: 600, epoch: 3 | loss: 0.2570502
	speed: 3.5817s/iter; left time: 30490.9321s
	iters: 700, epoch: 3 | loss: 0.2527155
	speed: 3.5805s/iter; left time: 30122.8742s
	iters: 800, epoch: 3 | loss: 0.2744907
	speed: 3.5811s/iter; left time: 29769.6220s
	iters: 900, epoch: 3 | loss: 0.2563585
	speed: 3.5799s/iter; left time: 29401.8270s
	iters: 1000, epoch: 3 | loss: 0.2429348
	speed: 3.3780s/iter; left time: 27406.0282s
	iters: 1100, epoch: 3 | loss: 0.2598273
	speed: 2.4658s/iter; left time: 19758.7269s
Epoch: 3 cost time: 3867.2767968177795
Epoch: 3, Steps: 1139 | Train Loss: 0.2559147 Vali Loss: 0.1634514 Test Loss: 0.1843726
Validation loss decreased (0.172374 --> 0.163451).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2585280
	speed: 6.2254s/iter; left time: 49018.9703s
	iters: 200, epoch: 4 | loss: 0.2537701
	speed: 2.4665s/iter; left time: 19174.5343s
	iters: 300, epoch: 4 | loss: 0.2349877
	speed: 2.4663s/iter; left time: 18926.5304s
	iters: 400, epoch: 4 | loss: 0.2037840
	speed: 2.4016s/iter; left time: 18189.4896s
	iters: 500, epoch: 4 | loss: 0.2598816
	speed: 2.2683s/iter; left time: 16953.3865s
	iters: 600, epoch: 4 | loss: 0.2470150
	speed: 2.3833s/iter; left time: 17574.6990s
	iters: 700, epoch: 4 | loss: 0.2315497
	speed: 2.4652s/iter; left time: 17931.5304s
	iters: 800, epoch: 4 | loss: 0.2679962
	speed: 2.4667s/iter; left time: 17696.4644s
	iters: 900, epoch: 4 | loss: 0.2317406
	speed: 2.4657s/iter; left time: 17442.5892s
	iters: 1000, epoch: 4 | loss: 0.2685534
	speed: 2.4665s/iter; left time: 17201.7067s
	iters: 1100, epoch: 4 | loss: 0.2521580
	speed: 2.4642s/iter; left time: 16939.1868s
Epoch: 4 cost time: 2772.764876127243
Epoch: 4, Steps: 1139 | Train Loss: 0.2533829 Vali Loss: 0.1608100 Test Loss: 0.1814493
Validation loss decreased (0.163451 --> 0.160810).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2645539
	speed: 6.2309s/iter; left time: 41965.0642s
	iters: 200, epoch: 5 | loss: 0.2355235
	speed: 2.4655s/iter; left time: 16358.4277s
	iters: 300, epoch: 5 | loss: 0.2140360
	speed: 2.4658s/iter; left time: 16114.2648s
	iters: 400, epoch: 5 | loss: 0.2399613
	speed: 2.4649s/iter; left time: 15861.5854s
	iters: 500, epoch: 5 | loss: 0.2482897
	speed: 2.4650s/iter; left time: 15615.8516s
	iters: 600, epoch: 5 | loss: 0.2526386
	speed: 2.4653s/iter; left time: 15371.1710s
	iters: 700, epoch: 5 | loss: 0.2835035
	speed: 2.3781s/iter; left time: 14589.5075s
	iters: 800, epoch: 5 | loss: 0.2257372
	speed: 2.2601s/iter; left time: 13639.5070s
	iters: 900, epoch: 5 | loss: 0.2365464
	speed: 2.4104s/iter; left time: 14305.9188s
	iters: 1000, epoch: 5 | loss: 0.2875867
	speed: 2.4652s/iter; left time: 14384.6364s
	iters: 1100, epoch: 5 | loss: 0.2764402
	speed: 2.4655s/iter; left time: 14139.9035s
Epoch: 5 cost time: 2772.009853363037
Epoch: 5, Steps: 1139 | Train Loss: 0.2520204 Vali Loss: 0.1600535 Test Loss: 0.1810887
Validation loss decreased (0.160810 --> 0.160054).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.2468041
	speed: 6.2239s/iter; left time: 34828.7989s
	iters: 200, epoch: 6 | loss: 0.2516118
	speed: 2.4658s/iter; left time: 13552.3005s
	iters: 300, epoch: 6 | loss: 0.2749099
	speed: 2.4644s/iter; left time: 13297.6924s
	iters: 400, epoch: 6 | loss: 0.2808548
	speed: 2.4659s/iter; left time: 13059.4613s
	iters: 500, epoch: 6 | loss: 0.2689104
	speed: 2.4669s/iter; left time: 12818.0801s
	iters: 600, epoch: 6 | loss: 0.2577510
	speed: 2.4651s/iter; left time: 12562.1198s
	iters: 700, epoch: 6 | loss: 0.2403581
	speed: 2.4661s/iter; left time: 12320.7486s
	iters: 800, epoch: 6 | loss: 0.2571254
	speed: 2.4654s/iter; left time: 12070.7039s
	iters: 900, epoch: 6 | loss: 0.2206767
	speed: 2.4641s/iter; left time: 11817.8798s
	iters: 1000, epoch: 6 | loss: 0.2609707
	speed: 2.3456s/iter; left time: 11014.8775s
	iters: 1100, epoch: 6 | loss: 0.3132351
	speed: 2.2752s/iter; left time: 10456.7607s
Epoch: 6 cost time: 2772.482686519623
Epoch: 6, Steps: 1139 | Train Loss: 0.2513304 Vali Loss: 0.1591964 Test Loss: 0.1795650
Validation loss decreased (0.160054 --> 0.159196).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.2743181
	speed: 6.1919s/iter; left time: 27597.3746s
	iters: 200, epoch: 7 | loss: 0.2289606
	speed: 2.4653s/iter; left time: 10741.0974s
	iters: 300, epoch: 7 | loss: 0.2524424
	speed: 2.4664s/iter; left time: 10499.3127s
	iters: 400, epoch: 7 | loss: 0.2807781
	speed: 2.4641s/iter; left time: 10243.4486s
	iters: 500, epoch: 7 | loss: 0.2501821
	speed: 2.4656s/iter; left time: 10003.0858s
	iters: 600, epoch: 7 | loss: 0.2458116
	speed: 2.4657s/iter; left time: 9756.6625s
	iters: 700, epoch: 7 | loss: 0.2544238
	speed: 2.4650s/iter; left time: 9507.3122s
	iters: 800, epoch: 7 | loss: 0.2414929
	speed: 2.4672s/iter; left time: 9269.3762s
	iters: 900, epoch: 7 | loss: 0.2301904
	speed: 2.4647s/iter; left time: 9013.4274s
	iters: 1000, epoch: 7 | loss: 0.2485033
	speed: 2.4655s/iter; left time: 8769.7525s
	iters: 1100, epoch: 7 | loss: 0.2292460
	speed: 2.4656s/iter; left time: 8523.5267s
Epoch: 7 cost time: 2806.7963478565216
Epoch: 7, Steps: 1139 | Train Loss: 0.2509177 Vali Loss: 0.1592148 Test Loss: 0.1798812
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.2596500
	speed: 5.9703s/iter; left time: 19809.6070s
	iters: 200, epoch: 8 | loss: 0.2568321
	speed: 2.2262s/iter; left time: 7163.8344s
	iters: 300, epoch: 8 | loss: 0.2173257
	speed: 2.0197s/iter; left time: 6297.4274s
	iters: 400, epoch: 8 | loss: 0.2715625
	speed: 2.5004s/iter; left time: 7546.1138s
	iters: 500, epoch: 8 | loss: 0.2481260
	speed: 2.5006s/iter; left time: 7296.7679s
	iters: 600, epoch: 8 | loss: 0.2558088
	speed: 2.4999s/iter; left time: 7044.5850s
	iters: 700, epoch: 8 | loss: 0.2748639
	speed: 2.5008s/iter; left time: 6797.2525s
	iters: 800, epoch: 8 | loss: 0.2455639
	speed: 2.4994s/iter; left time: 6543.4703s
	iters: 900, epoch: 8 | loss: 0.2490485
	speed: 2.5002s/iter; left time: 6295.3960s
	iters: 1000, epoch: 8 | loss: 0.2502836
	speed: 2.5010s/iter; left time: 6047.4526s
	iters: 1100, epoch: 8 | loss: 0.2586440
	speed: 2.5003s/iter; left time: 5795.5940s
Epoch: 8 cost time: 2747.7301592826843
Epoch: 8, Steps: 1139 | Train Loss: 0.2506972 Vali Loss: 0.1592519 Test Loss: 0.1800570
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.2419635
	speed: 6.2998s/iter; left time: 13727.2303s
	iters: 200, epoch: 9 | loss: 0.2413998
	speed: 2.4997s/iter; left time: 5196.9065s
	iters: 300, epoch: 9 | loss: 0.2600816
	speed: 2.4994s/iter; left time: 4946.3236s
	iters: 400, epoch: 9 | loss: 0.2716661
	speed: 2.4997s/iter; left time: 4696.8956s
	iters: 500, epoch: 9 | loss: 0.2507571
	speed: 2.4995s/iter; left time: 4446.5318s
	iters: 600, epoch: 9 | loss: 0.2466055
	speed: 2.4997s/iter; left time: 4197.0406s
	iters: 700, epoch: 9 | loss: 0.2875139
	speed: 2.4996s/iter; left time: 3946.8940s
	iters: 800, epoch: 9 | loss: 0.2239460
	speed: 2.4986s/iter; left time: 3695.3595s
	iters: 900, epoch: 9 | loss: 0.2707089
	speed: 2.4992s/iter; left time: 3446.4328s
	iters: 1000, epoch: 9 | loss: 0.2395290
	speed: 2.4988s/iter; left time: 3195.9541s
	iters: 1100, epoch: 9 | loss: 0.2619624
	speed: 2.4987s/iter; left time: 2945.9533s
Epoch: 9 cost time: 2845.24493765831
Epoch: 9, Steps: 1139 | Train Loss: 0.2506459 Vali Loss: 0.1592093 Test Loss: 0.1795277
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_ECL_96_96_AGPTp_2048_fixedFalse_0.0001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5165
test shape: (5165, 96, 321) (5165, 96, 321)
test shape: (5165, 96, 321) (5165, 96, 321)
mse:0.17954635620117188, mae:0.26669958233833313
================================================================================
Model Profiling Summary
Total Params             : 6,982,499
Inference Time (s)       : 0.549537
GPU Mem Footprint (MB)   : 92.62
Peak Mem (MB)            : 2315.14
================================================================================
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_192          Model:              AGPTp               

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_192_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 18125
val 2441
test 5069
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 194, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 344, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 275, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 70, in forward
    V = torch.einsum("bhls,bshd->blhd", A, values)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/functional.py", line 386, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 10.12 MiB is free. Process 4074226 has 30.04 GiB memory in use. Process 492650 has 49.08 GiB memory in use. Of the allocated memory 48.40 GiB is allocated by PyTorch, and 196.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_336          Model:              AGPTp               

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_336_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17981
val 2297
test 4925
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 194, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 344, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 275, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 70, in forward
    V = torch.einsum("bhls,bshd->blhd", A, values)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/functional.py", line 386, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 4.12 MiB is free. Process 4074226 has 30.04 GiB memory in use. Process 494275 has 49.09 GiB memory in use. Of the allocated memory 48.41 GiB is allocated by PyTorch, and 191.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_720          Model:              AGPTp               

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_720_AGPTp_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17597
val 1913
test 4541
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 194, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 344, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPTp.py", line 275, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 69, in forward
    A = self.dropout(torch.softmax(scale * scores, dim=-1))
                                   ~~~~~~^~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 14.12 MiB is free. Process 4074226 has 30.04 GiB memory in use. Process 496038 has 49.08 GiB memory in use. Of the allocated memory 48.36 GiB is allocated by PyTorch, and 237.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
