Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          Exp_TimeFilter      Is Training:        1                   
  Model ID:           PEMS03              Model:              TimeFilter          

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS03.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             358                 Dec In:             358                 
  C Out:              358                 d model:            32                  
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : Exp_TimeFilter_PEMS03_TimeFilter_128_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 15617
val 5135
test 5135
Traceback (most recent call last):
  File "/root/daye/AGPT/run.py", line 203, in <module>
    exp.train(setting)
  File "/root/daye/AGPT/exp/exp_TimeFilter.py", line 146, in train
    outputs, moe_loss = self.model(batch_x, self.masks, is_training=True)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/daye/AGPT/models/TimeFilter.py", line 73, in forward
    x, moe_loss = self.backbone(x, masks, self.alpha, is_training)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/daye/AGPT/layers/TimeFilter_layers.py", line 277, in forward
    x, loss = block(x, masks, alpha, is_training)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/daye/AGPT/layers/TimeFilter_layers.py", line 255, in forward
    out, loss = self.gnn(self.norm1(x), masks, alpha, is_training)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/daye/AGPT/layers/TimeFilter_layers.py", line 233, in forward
    adj = self.dropout(adj)
          ^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/functional.py", line 1268, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.40 GiB. GPU 0 has a total capacity of 79.25 GiB of which 3.61 GiB is free. Process 1872957 has 686.00 MiB memory in use. Process 503296 has 11.85 GiB memory in use. Process 1236997 has 570.00 MiB memory in use. Process 1254455 has 3.61 GiB memory in use. Process 1272779 has 2.98 GiB memory in use. Including non-PyTorch memory, this process has 55.94 GiB memory in use. Of the allocated memory 55.36 GiB is allocated by PyTorch, and 91.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          Exp_TimeFilter      Is Training:        1                   
  Model ID:           PEMS04              Model:              TimeFilter          

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS04.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             307                 Dec In:             307                 
  C Out:              307                 d model:            32                  
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               128                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
Traceback (most recent call last):
  File "/root/daye/AGPT/run.py", line 198, in <module>
    exp = Exp(args)  # set experiments
          ^^^^^^^^^
  File "/root/daye/AGPT/exp/exp_TimeFilter.py", line 23, in __init__
    self.masks = self._get_mask()
                 ^^^^^^^^^^^^^^^^
  File "/root/daye/AGPT/exp/exp_TimeFilter.py", line 55, in _get_mask
    ST = torch.ones(L).to(dtype).to(self.device) - S - T
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
