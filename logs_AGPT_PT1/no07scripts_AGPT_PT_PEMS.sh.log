Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS03              Model:              AGPT_PT             

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS03.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             358                 Dec In:             358                 
  C Out:              358                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS03_AGPT_PT_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 15617
val 5135
test 5135
	iters: 100, epoch: 1 | loss: 0.2374552
	speed: 4.4024s/iter; left time: 21091.6794s
	iters: 200, epoch: 1 | loss: 0.2246287
	speed: 4.3051s/iter; left time: 20195.4574s
	iters: 300, epoch: 1 | loss: 0.2341384
	speed: 3.8781s/iter; left time: 17804.1674s
	iters: 400, epoch: 1 | loss: 0.1877492
	speed: 3.5761s/iter; left time: 16060.0779s
Epoch: 1 cost time: 1950.0407905578613
Epoch: 1, Steps: 489 | Train Loss: 0.2386828 Vali Loss: 0.1037097 Test Loss: 0.1034468
Validation loss decreased (inf --> 0.103710).  Saving model ...
Updating learning rate to 0.003
	iters: 100, epoch: 2 | loss: 0.2044537
	speed: 10.8033s/iter; left time: 46475.6790s
	iters: 200, epoch: 2 | loss: 0.1995692
	speed: 2.7972s/iter; left time: 11754.0379s
	iters: 300, epoch: 2 | loss: 0.1685405
	speed: 3.0323s/iter; left time: 12438.5641s
	iters: 400, epoch: 2 | loss: 0.2344244
	speed: 3.0877s/iter; left time: 12356.8373s
Epoch: 2 cost time: 1473.9113788604736
Epoch: 2, Steps: 489 | Train Loss: 0.2042965 Vali Loss: 0.1116931 Test Loss: 0.1104941
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0015
	iters: 100, epoch: 3 | loss: 0.1997928
	speed: 8.9487s/iter; left time: 34121.3201s
	iters: 200, epoch: 3 | loss: 0.2495696
	speed: 2.9018s/iter; left time: 10774.2105s
	iters: 300, epoch: 3 | loss: 0.2032249
	speed: 2.5620s/iter; left time: 9256.6557s
	iters: 400, epoch: 3 | loss: 0.1866009
	speed: 2.3329s/iter; left time: 8195.3838s
Epoch: 3 cost time: 1302.5122773647308
Epoch: 3, Steps: 489 | Train Loss: 0.1958291 Vali Loss: 0.0945017 Test Loss: 0.0955951
Validation loss decreased (0.103710 --> 0.094502).  Saving model ...
Updating learning rate to 0.00075
	iters: 100, epoch: 4 | loss: 0.1629508
	speed: 7.7282s/iter; left time: 25688.4737s
	iters: 200, epoch: 4 | loss: 0.2406505
	speed: 2.5778s/iter; left time: 8310.8573s
	iters: 300, epoch: 4 | loss: 0.2109042
	speed: 2.6033s/iter; left time: 8132.7584s
	iters: 400, epoch: 4 | loss: 0.1683166
	speed: 2.7320s/iter; left time: 8261.4673s
Epoch: 4 cost time: 1320.7981493473053
Epoch: 4, Steps: 489 | Train Loss: 0.1914966 Vali Loss: 0.1046680 Test Loss: 0.1055578
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.000375
	iters: 100, epoch: 5 | loss: 0.1780004
	speed: 7.2879s/iter; left time: 20661.1112s
	iters: 200, epoch: 5 | loss: 0.1709917
	speed: 2.0517s/iter; left time: 5611.3508s
	iters: 300, epoch: 5 | loss: 0.1797640
	speed: 1.8527s/iter; left time: 4881.7588s
	iters: 400, epoch: 5 | loss: 0.1853989
	speed: 0.9616s/iter; left time: 2437.7607s
Epoch: 5 cost time: 775.5006475448608
Epoch: 5, Steps: 489 | Train Loss: 0.1896408 Vali Loss: 0.0925895 Test Loss: 0.0926338
Validation loss decreased (0.094502 --> 0.092590).  Saving model ...
Updating learning rate to 0.0001875
	iters: 100, epoch: 6 | loss: 0.1723406
	speed: 2.8590s/iter; left time: 6707.1218s
	iters: 200, epoch: 6 | loss: 0.1723969
	speed: 0.9608s/iter; left time: 2157.9779s
	iters: 300, epoch: 6 | loss: 0.1494986
	speed: 0.9605s/iter; left time: 2061.1272s
	iters: 400, epoch: 6 | loss: 0.1979339
	speed: 0.9609s/iter; left time: 1965.9222s
Epoch: 6 cost time: 469.25829339027405
Epoch: 6, Steps: 489 | Train Loss: 0.1881221 Vali Loss: 0.0929276 Test Loss: 0.0946862
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.375e-05
	iters: 100, epoch: 7 | loss: 0.2128341
	speed: 2.8563s/iter; left time: 5304.2180s
	iters: 200, epoch: 7 | loss: 0.2032108
	speed: 0.9608s/iter; left time: 1688.2122s
	iters: 300, epoch: 7 | loss: 0.1782813
	speed: 0.9604s/iter; left time: 1591.3510s
	iters: 400, epoch: 7 | loss: 0.1595611
	speed: 0.9607s/iter; left time: 1495.8396s
Epoch: 7 cost time: 469.1162464618683
Epoch: 7, Steps: 489 | Train Loss: 0.1872306 Vali Loss: 0.0891123 Test Loss: 0.0905432
Validation loss decreased (0.092590 --> 0.089112).  Saving model ...
Updating learning rate to 4.6875e-05
	iters: 100, epoch: 8 | loss: 0.2049649
	speed: 2.8570s/iter; left time: 3908.3087s
	iters: 200, epoch: 8 | loss: 0.2209618
	speed: 0.9612s/iter; left time: 1218.7673s
	iters: 300, epoch: 8 | loss: 0.1905795
	speed: 0.9612s/iter; left time: 1122.7192s
	iters: 400, epoch: 8 | loss: 0.1731641
	speed: 0.9611s/iter; left time: 1026.4434s
Epoch: 8 cost time: 469.4126868247986
Epoch: 8, Steps: 489 | Train Loss: 0.1868062 Vali Loss: 0.0875278 Test Loss: 0.0892299
Validation loss decreased (0.089112 --> 0.087528).  Saving model ...
Updating learning rate to 2.34375e-05
	iters: 100, epoch: 9 | loss: 0.1609743
	speed: 2.8578s/iter; left time: 2512.0218s
	iters: 200, epoch: 9 | loss: 0.1833435
	speed: 0.9610s/iter; left time: 748.6127s
	iters: 300, epoch: 9 | loss: 0.1896209
	speed: 0.9609s/iter; left time: 652.4218s
	iters: 400, epoch: 9 | loss: 0.2080585
	speed: 0.9612s/iter; left time: 556.5292s
Epoch: 9 cost time: 469.26555132865906
Epoch: 9, Steps: 489 | Train Loss: 0.1864871 Vali Loss: 0.0873829 Test Loss: 0.0889986
Validation loss decreased (0.087528 --> 0.087383).  Saving model ...
Updating learning rate to 1.171875e-05
	iters: 100, epoch: 10 | loss: 0.2232554
	speed: 2.8542s/iter; left time: 1113.1471s
	iters: 200, epoch: 10 | loss: 0.1580969
	speed: 0.9609s/iter; left time: 278.6533s
	iters: 300, epoch: 10 | loss: 0.1986739
	speed: 0.9607s/iter; left time: 182.5262s
	iters: 400, epoch: 10 | loss: 0.1570487
	speed: 0.9609s/iter; left time: 86.4825s
Epoch: 10 cost time: 469.16556763648987
Epoch: 10, Steps: 489 | Train Loss: 0.1863654 Vali Loss: 0.0860089 Test Loss: 0.0874688
Validation loss decreased (0.087383 --> 0.086009).  Saving model ...
Updating learning rate to 5.859375e-06
>>>>>>>testing : AGPT_loss_PEMS03_AGPT_PT_256_fixedFalse_0.003_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5135
test shape: (5135, 12, 358) (5135, 12, 358)
test shape: (5135, 12, 358) (5135, 12, 358)
mse:0.08751893788576126, mae:0.20093579590320587
================================================================================
Model Profiling Summary
Total Params             : 704,527
Inference Time (s)       : 0.316221
GPU Mem Footprint (MB)   : 32.08
Peak Mem (MB)            : 926.38
================================================================================
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS04              Model:              AGPT_PT             

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS04.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             307                 Dec In:             307                 
  C Out:              307                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS04_AGPT_PT_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10088
val 3291
test 3292
	iters: 100, epoch: 1 | loss: 0.2121208
	speed: 0.7952s/iter; left time: 2434.0176s
	iters: 200, epoch: 1 | loss: 0.2676040
	speed: 0.7944s/iter; left time: 2352.2016s
	iters: 300, epoch: 1 | loss: 0.2414667
	speed: 0.7950s/iter; left time: 2274.4733s
Epoch: 1 cost time: 251.07719111442566
Epoch: 1, Steps: 316 | Train Loss: 0.2602143 Vali Loss: 0.1297187 Test Loss: 0.1213701
Validation loss decreased (inf --> 0.129719).  Saving model ...
Updating learning rate to 0.003
	iters: 100, epoch: 2 | loss: 0.2269966
	speed: 1.4949s/iter; left time: 4103.6125s
	iters: 200, epoch: 2 | loss: 0.2544973
	speed: 0.7943s/iter; left time: 2101.0413s
	iters: 300, epoch: 2 | loss: 0.1863666
	speed: 0.7941s/iter; left time: 2021.1116s
Epoch: 2 cost time: 250.5843267440796
Epoch: 2, Steps: 316 | Train Loss: 0.2151647 Vali Loss: 0.1319040 Test Loss: 0.1239463
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0015
	iters: 100, epoch: 3 | loss: 0.1863805
	speed: 1.4952s/iter; left time: 3631.7673s
	iters: 200, epoch: 3 | loss: 0.2044005
	speed: 0.7944s/iter; left time: 1850.0984s
	iters: 300, epoch: 3 | loss: 0.1780999
	speed: 0.7946s/iter; left time: 1771.2277s
Epoch: 3 cost time: 250.6224663257599
Epoch: 3, Steps: 316 | Train Loss: 0.2069789 Vali Loss: 0.1182334 Test Loss: 0.1125284
Validation loss decreased (0.129719 --> 0.118233).  Saving model ...
Updating learning rate to 0.00075
	iters: 100, epoch: 4 | loss: 0.2087720
	speed: 1.4979s/iter; left time: 3164.9579s
	iters: 200, epoch: 4 | loss: 0.1899158
	speed: 0.7947s/iter; left time: 1599.7803s
	iters: 300, epoch: 4 | loss: 0.2138795
	speed: 0.7943s/iter; left time: 1519.4808s
Epoch: 4 cost time: 250.69840168952942
Epoch: 4, Steps: 316 | Train Loss: 0.2037787 Vali Loss: 0.1150495 Test Loss: 0.1101063
Validation loss decreased (0.118233 --> 0.115049).  Saving model ...
Updating learning rate to 0.000375
	iters: 100, epoch: 5 | loss: 0.2046950
	speed: 1.4966s/iter; left time: 2689.3013s
	iters: 200, epoch: 5 | loss: 0.2094186
	speed: 0.7940s/iter; left time: 1347.4694s
	iters: 300, epoch: 5 | loss: 0.1796556
	speed: 0.7941s/iter; left time: 1268.2396s
Epoch: 5 cost time: 250.5340211391449
Epoch: 5, Steps: 316 | Train Loss: 0.2026786 Vali Loss: 0.1171748 Test Loss: 0.1119645
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0001875
	iters: 100, epoch: 6 | loss: 0.2166879
	speed: 1.4964s/iter; left time: 2216.1792s
	iters: 200, epoch: 6 | loss: 0.2294878
	speed: 0.7943s/iter; left time: 1096.8924s
	iters: 300, epoch: 6 | loss: 0.2029324
	speed: 0.7944s/iter; left time: 1017.6196s
Epoch: 6 cost time: 250.6064956188202
Epoch: 6, Steps: 316 | Train Loss: 0.2015661 Vali Loss: 0.1155224 Test Loss: 0.1104162
EarlyStopping counter: 2 out of 10
Updating learning rate to 9.375e-05
	iters: 100, epoch: 7 | loss: 0.1808753
	speed: 1.4971s/iter; left time: 1744.0868s
	iters: 200, epoch: 7 | loss: 0.2340765
	speed: 0.7944s/iter; left time: 846.0379s
	iters: 300, epoch: 7 | loss: 0.2065550
	speed: 0.7947s/iter; left time: 766.8489s
Epoch: 7 cost time: 250.6561462879181
Epoch: 7, Steps: 316 | Train Loss: 0.2013085 Vali Loss: 0.1160310 Test Loss: 0.1106951
EarlyStopping counter: 3 out of 10
Updating learning rate to 4.6875e-05
	iters: 100, epoch: 8 | loss: 0.2345336
	speed: 1.4965s/iter; left time: 1270.4995s
	iters: 200, epoch: 8 | loss: 0.1684248
	speed: 0.7951s/iter; left time: 595.5520s
	iters: 300, epoch: 8 | loss: 0.1679110
	speed: 0.7949s/iter; left time: 515.8778s
Epoch: 8 cost time: 250.78456473350525
Epoch: 8, Steps: 316 | Train Loss: 0.2009988 Vali Loss: 0.1151120 Test Loss: 0.1096238
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.34375e-05
	iters: 100, epoch: 9 | loss: 0.1811263
	speed: 1.4997s/iter; left time: 799.3402s
	iters: 200, epoch: 9 | loss: 0.2233855
	speed: 0.7942s/iter; left time: 343.9040s
	iters: 300, epoch: 9 | loss: 0.1772077
	speed: 0.7942s/iter; left time: 264.4583s
Epoch: 9 cost time: 250.62426733970642
Epoch: 9, Steps: 316 | Train Loss: 0.2008290 Vali Loss: 0.1166443 Test Loss: 0.1111363
EarlyStopping counter: 5 out of 10
Updating learning rate to 1.171875e-05
	iters: 100, epoch: 10 | loss: 0.1739835
	speed: 1.4979s/iter; left time: 325.0541s
	iters: 200, epoch: 10 | loss: 0.1923304
	speed: 0.7943s/iter; left time: 92.9372s
	iters: 300, epoch: 10 | loss: 0.1747741
	speed: 0.7940s/iter; left time: 13.4984s
Epoch: 10 cost time: 250.68457674980164
Epoch: 10, Steps: 316 | Train Loss: 0.2007234 Vali Loss: 0.1150412 Test Loss: 0.1097575
Validation loss decreased (0.115049 --> 0.115041).  Saving model ...
Updating learning rate to 5.859375e-06
>>>>>>>testing : AGPT_loss_PEMS04_AGPT_PT_256_fixedFalse_0.003_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3292
test shape: (3292, 12, 307) (3292, 12, 307)
test shape: (3292, 12, 307) (3292, 12, 307)
mse:0.10980910807847977, mae:0.2283116579055786
================================================================================
Model Profiling Summary
Total Params             : 704,527
Inference Time (s)       : 0.271896
GPU Mem Footprint (MB)   : 32.60
Peak Mem (MB)            : 799.50
================================================================================
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS07              Model:              AGPT_PT             

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS07.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             883                 Dec In:             883                 
  C Out:              883                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS07_AGPT_PT_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16827
val 5538
test 5538
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 180, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 344, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 275, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 104.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 29.62 MiB is free. Process 696223 has 79.11 GiB memory in use. Of the allocated memory 78.12 GiB is allocated by PyTorch, and 516.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS08              Model:              AGPT_PT             

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS08.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             170                 Dec In:             170                 
  C Out:              170                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS08_AGPT_PT_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10606
val 3464
test 3465
	iters: 100, epoch: 1 | loss: 0.2355055
	speed: 0.4710s/iter; left time: 1517.0137s
	iters: 200, epoch: 1 | loss: 0.2441874
	speed: 0.4675s/iter; left time: 1458.9449s
	iters: 300, epoch: 1 | loss: 0.2142580
	speed: 0.4675s/iter; left time: 1412.3685s
Epoch: 1 cost time: 155.58308291435242
Epoch: 1, Steps: 332 | Train Loss: 0.2429530 Vali Loss: 0.1523303 Test Loss: 0.1442741
Validation loss decreased (inf --> 0.152330).  Saving model ...
Updating learning rate to 0.003
	iters: 100, epoch: 2 | loss: 0.2030549
	speed: 0.9669s/iter; left time: 2793.4909s
	iters: 200, epoch: 2 | loss: 0.2153655
	speed: 0.4676s/iter; left time: 1304.1339s
	iters: 300, epoch: 2 | loss: 0.1780742
	speed: 0.4677s/iter; left time: 1257.5130s
Epoch: 2 cost time: 155.15817761421204
Epoch: 2, Steps: 332 | Train Loss: 0.2091718 Vali Loss: 0.1205223 Test Loss: 0.1104812
Validation loss decreased (0.152330 --> 0.120522).  Saving model ...
Updating learning rate to 0.0015
	iters: 100, epoch: 3 | loss: 0.1949063
	speed: 0.9688s/iter; left time: 2477.3182s
	iters: 200, epoch: 3 | loss: 0.2140193
	speed: 0.4674s/iter; left time: 1148.3444s
	iters: 300, epoch: 3 | loss: 0.2117225
	speed: 0.4676s/iter; left time: 1102.0195s
Epoch: 3 cost time: 155.13594436645508
Epoch: 3, Steps: 332 | Train Loss: 0.1994828 Vali Loss: 0.1156414 Test Loss: 0.1048861
Validation loss decreased (0.120522 --> 0.115641).  Saving model ...
Updating learning rate to 0.00075
	iters: 100, epoch: 4 | loss: 0.1995814
	speed: 0.9686s/iter; left time: 2155.0707s
	iters: 200, epoch: 4 | loss: 0.2537027
	speed: 0.4675s/iter; left time: 993.3428s
	iters: 300, epoch: 4 | loss: 0.2126117
	speed: 0.4678s/iter; left time: 947.3391s
Epoch: 4 cost time: 155.24389266967773
Epoch: 4, Steps: 332 | Train Loss: 0.1969436 Vali Loss: 0.1198487 Test Loss: 0.1111456
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.000375
	iters: 100, epoch: 5 | loss: 0.2216986
	speed: 0.9680s/iter; left time: 1832.3903s
	iters: 200, epoch: 5 | loss: 0.1812014
	speed: 0.4674s/iter; left time: 838.0254s
	iters: 300, epoch: 5 | loss: 0.1965212
	speed: 0.4676s/iter; left time: 791.6270s
Epoch: 5 cost time: 155.12079644203186
Epoch: 5, Steps: 332 | Train Loss: 0.1951940 Vali Loss: 0.1109338 Test Loss: 0.1010625
Validation loss decreased (0.115641 --> 0.110934).  Saving model ...
Updating learning rate to 0.0001875
	iters: 100, epoch: 6 | loss: 0.1931437
	speed: 0.9683s/iter; left time: 1511.4994s
	iters: 200, epoch: 6 | loss: 0.1961643
	speed: 0.4674s/iter; left time: 682.8329s
	iters: 300, epoch: 6 | loss: 0.1806930
	speed: 0.4673s/iter; left time: 636.0575s
Epoch: 6 cost time: 155.13215136528015
Epoch: 6, Steps: 332 | Train Loss: 0.1939009 Vali Loss: 0.1146456 Test Loss: 0.1058695
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.375e-05
	iters: 100, epoch: 7 | loss: 0.1966006
	speed: 0.9672s/iter; left time: 1188.6956s
	iters: 200, epoch: 7 | loss: 0.1904607
	speed: 0.4674s/iter; left time: 527.6921s
	iters: 300, epoch: 7 | loss: 0.2231561
	speed: 0.4672s/iter; left time: 480.7022s
Epoch: 7 cost time: 155.10141324996948
Epoch: 7, Steps: 332 | Train Loss: 0.1933362 Vali Loss: 0.1136962 Test Loss: 0.1047026
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.6875e-05
	iters: 100, epoch: 8 | loss: 0.1988800
	speed: 0.9687s/iter; left time: 868.9036s
	iters: 200, epoch: 8 | loss: 0.2003829
	speed: 0.4674s/iter; left time: 372.5514s
	iters: 300, epoch: 8 | loss: 0.1810860
	speed: 0.4677s/iter; left time: 325.9932s
Epoch: 8 cost time: 155.14968943595886
Epoch: 8, Steps: 332 | Train Loss: 0.1927454 Vali Loss: 0.1134233 Test Loss: 0.1039926
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.34375e-05
	iters: 100, epoch: 9 | loss: 0.2212916
	speed: 0.9686s/iter; left time: 547.2698s
	iters: 200, epoch: 9 | loss: 0.2085714
	speed: 0.4676s/iter; left time: 217.4119s
	iters: 300, epoch: 9 | loss: 0.2123641
	speed: 0.4671s/iter; left time: 170.5069s
Epoch: 9 cost time: 155.19444131851196
Epoch: 9, Steps: 332 | Train Loss: 0.1927600 Vali Loss: 0.1116932 Test Loss: 0.1024113
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.171875e-05
	iters: 100, epoch: 10 | loss: 0.1987920
	speed: 0.9673s/iter; left time: 225.3710s
	iters: 200, epoch: 10 | loss: 0.1662263
	speed: 0.4676s/iter; left time: 62.1864s
	iters: 300, epoch: 10 | loss: 0.1800559
	speed: 0.4674s/iter; left time: 15.4230s
Epoch: 10 cost time: 155.1013720035553
Epoch: 10, Steps: 332 | Train Loss: 0.1927242 Vali Loss: 0.1118618 Test Loss: 0.1024680
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.859375e-06
>>>>>>>testing : AGPT_loss_PEMS08_AGPT_PT_256_fixedFalse_0.003_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3465
test shape: (3465, 12, 170) (3465, 12, 170)
test shape: (3465, 12, 170) (3465, 12, 170)
mse:0.1013362780213356, mae:0.21693506836891174
================================================================================
Model Profiling Summary
Total Params             : 704,527
Inference Time (s)       : 0.155135
GPU Mem Footprint (MB)   : 27.47
Peak Mem (MB)            : 452.14
================================================================================
