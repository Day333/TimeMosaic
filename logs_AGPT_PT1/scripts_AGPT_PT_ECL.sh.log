Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_96           Model:              AGPT_PT             

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_96_AGPT_PT_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 18221
val 2537
test 5165
	iters: 100, epoch: 1 | loss: 0.2903271
	speed: 2.4675s/iter; left time: 27860.4953s
	iters: 200, epoch: 1 | loss: 0.2780381
	speed: 2.4772s/iter; left time: 27721.8341s
	iters: 300, epoch: 1 | loss: 0.2754116
	speed: 2.4793s/iter; left time: 27498.3899s
	iters: 400, epoch: 1 | loss: 0.2972430
	speed: 2.4816s/iter; left time: 27275.7540s
	iters: 500, epoch: 1 | loss: 0.2576621
	speed: 2.4807s/iter; left time: 27016.9349s
	iters: 600, epoch: 1 | loss: 0.2573299
	speed: 2.4791s/iter; left time: 26751.7115s
	iters: 700, epoch: 1 | loss: 0.2810158
	speed: 2.4812s/iter; left time: 26526.1215s
	iters: 800, epoch: 1 | loss: 0.2368291
	speed: 2.4801s/iter; left time: 26267.2657s
	iters: 900, epoch: 1 | loss: 0.2672109
	speed: 2.2953s/iter; left time: 24080.0458s
	iters: 1000, epoch: 1 | loss: 0.2669228
	speed: 2.3057s/iter; left time: 23958.8578s
	iters: 1100, epoch: 1 | loss: 0.2847316
	speed: 2.4748s/iter; left time: 25468.1157s
Epoch: 1 cost time: 2787.069376707077
Epoch: 1, Steps: 1139 | Train Loss: 0.2845324 Vali Loss: 0.1723741 Test Loss: 0.1950275
Validation loss decreased (inf --> 0.172374).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.2723579
	speed: 6.2294s/iter; left time: 63240.7529s
	iters: 200, epoch: 2 | loss: 0.2508300
	speed: 2.4831s/iter; left time: 24959.6224s
	iters: 300, epoch: 2 | loss: 0.2769425
	speed: 2.4817s/iter; left time: 24698.2967s
	iters: 400, epoch: 2 | loss: 0.2916026
	speed: 2.4829s/iter; left time: 24461.2968s
	iters: 500, epoch: 2 | loss: 0.2531412
	speed: 2.4823s/iter; left time: 24207.6037s
	iters: 600, epoch: 2 | loss: 0.2448927
	speed: 2.4810s/iter; left time: 23946.6874s
	iters: 700, epoch: 2 | loss: 0.2534389
	speed: 2.4802s/iter; left time: 23691.1821s
	iters: 800, epoch: 2 | loss: 0.2561986
	speed: 2.4806s/iter; left time: 23446.2965s
	iters: 900, epoch: 2 | loss: 0.2616090
	speed: 2.4791s/iter; left time: 23184.9633s
	iters: 1000, epoch: 2 | loss: 0.2551846
	speed: 2.4828s/iter; left time: 22970.8130s
	iters: 1100, epoch: 2 | loss: 0.2589238
	speed: 2.4793s/iter; left time: 22690.8173s
Epoch: 2 cost time: 2817.3740899562836
Epoch: 2, Steps: 1139 | Train Loss: 0.2617990 Vali Loss: 0.1733325 Test Loss: 0.1949963
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
	iters: 100, epoch: 3 | loss: 0.2244420
	speed: 5.8956s/iter; left time: 53136.6721s
	iters: 200, epoch: 3 | loss: 0.2736953
	speed: 2.4824s/iter; left time: 22125.3621s
	iters: 300, epoch: 3 | loss: 0.2534456
	speed: 2.4793s/iter; left time: 21849.9767s
	iters: 400, epoch: 3 | loss: 0.2816603
	speed: 2.4780s/iter; left time: 21590.5653s
	iters: 500, epoch: 3 | loss: 0.2360699
	speed: 2.4809s/iter; left time: 21367.8286s
	iters: 600, epoch: 3 | loss: 0.2570502
	speed: 2.4816s/iter; left time: 21125.4665s
	iters: 700, epoch: 3 | loss: 0.2527155
	speed: 2.4800s/iter; left time: 20864.5411s
	iters: 800, epoch: 3 | loss: 0.2744907
	speed: 2.4803s/iter; left time: 20618.5234s
	iters: 900, epoch: 3 | loss: 0.2563585
	speed: 2.4805s/iter; left time: 20372.1694s
	iters: 1000, epoch: 3 | loss: 0.2429348
	speed: 2.4785s/iter; left time: 20107.9334s
	iters: 1100, epoch: 3 | loss: 0.2598273
	speed: 2.4815s/iter; left time: 19883.9967s
Epoch: 3 cost time: 2818.2684893608093
Epoch: 3, Steps: 1139 | Train Loss: 0.2559147 Vali Loss: 0.1634514 Test Loss: 0.1843726
Validation loss decreased (0.172374 --> 0.163451).  Saving model ...
Updating learning rate to 2.5e-05
	iters: 100, epoch: 4 | loss: 0.2585280
	speed: 6.2488s/iter; left time: 49202.8263s
	iters: 200, epoch: 4 | loss: 0.2537701
	speed: 2.3346s/iter; left time: 18149.0110s
	iters: 300, epoch: 4 | loss: 0.2349877
	speed: 2.2937s/iter; left time: 17601.6838s
	iters: 400, epoch: 4 | loss: 0.2037840
	speed: 2.2169s/iter; left time: 16790.7940s
	iters: 500, epoch: 4 | loss: 0.2598816
	speed: 2.0786s/iter; left time: 15535.4005s
	iters: 600, epoch: 4 | loss: 0.2470150
	speed: 2.5193s/iter; left time: 18577.3026s
	iters: 700, epoch: 4 | loss: 0.2315497
	speed: 2.5185s/iter; left time: 18319.7632s
	iters: 800, epoch: 4 | loss: 0.2679962
	speed: 2.5184s/iter; left time: 18067.0340s
	iters: 900, epoch: 4 | loss: 0.2317406
	speed: 2.5183s/iter; left time: 17814.2327s
	iters: 1000, epoch: 4 | loss: 0.2685534
	speed: 2.5195s/iter; left time: 17570.6920s
	iters: 1100, epoch: 4 | loss: 0.2521580
	speed: 2.5170s/iter; left time: 17301.8389s
Epoch: 4 cost time: 2748.0972785949707
Epoch: 4, Steps: 1139 | Train Loss: 0.2533829 Vali Loss: 0.1608100 Test Loss: 0.1814493
Validation loss decreased (0.163451 --> 0.160810).  Saving model ...
Updating learning rate to 1.25e-05
	iters: 100, epoch: 5 | loss: 0.2645539
	speed: 6.3473s/iter; left time: 42749.2716s
	iters: 200, epoch: 5 | loss: 0.2355235
	speed: 2.5167s/iter; left time: 16698.0268s
	iters: 300, epoch: 5 | loss: 0.2140360
	speed: 2.5154s/iter; left time: 16438.2556s
	iters: 400, epoch: 5 | loss: 0.2399613
	speed: 2.5159s/iter; left time: 16189.8837s
	iters: 500, epoch: 5 | loss: 0.2482897
	speed: 2.5165s/iter; left time: 15942.0047s
	iters: 600, epoch: 5 | loss: 0.2526386
	speed: 2.5172s/iter; left time: 15694.5595s
	iters: 700, epoch: 5 | loss: 0.2835035
	speed: 2.5166s/iter; left time: 15439.5875s
	iters: 800, epoch: 5 | loss: 0.2257372
	speed: 2.5167s/iter; left time: 15188.1812s
	iters: 900, epoch: 5 | loss: 0.2365464
	speed: 2.5179s/iter; left time: 14943.9668s
	iters: 1000, epoch: 5 | loss: 0.2875867
	speed: 2.5165s/iter; left time: 14684.0621s
	iters: 1100, epoch: 5 | loss: 0.2764402
	speed: 2.5167s/iter; left time: 14433.1851s
Epoch: 5 cost time: 2864.8946561813354
Epoch: 5, Steps: 1139 | Train Loss: 0.2520204 Vali Loss: 0.1600535 Test Loss: 0.1810887
Validation loss decreased (0.160810 --> 0.160054).  Saving model ...
Updating learning rate to 6.25e-06
	iters: 100, epoch: 6 | loss: 0.2468041
	speed: 6.3362s/iter; left time: 35457.4361s
	iters: 200, epoch: 6 | loss: 0.2516118
	speed: 2.5173s/iter; left time: 13835.1891s
	iters: 300, epoch: 6 | loss: 0.2749099
	speed: 2.5174s/iter; left time: 13583.8854s
	iters: 400, epoch: 6 | loss: 0.2808548
	speed: 2.5175s/iter; left time: 13332.4360s
	iters: 500, epoch: 6 | loss: 0.2689104
	speed: 2.5185s/iter; left time: 13086.3439s
	iters: 600, epoch: 6 | loss: 0.2577510
	speed: 2.4959s/iter; left time: 12719.0274s
	iters: 700, epoch: 6 | loss: 0.2403581
	speed: 2.4071s/iter; left time: 12025.9585s
	iters: 800, epoch: 6 | loss: 0.2571254
	speed: 2.3872s/iter; left time: 11687.5957s
	iters: 900, epoch: 6 | loss: 0.2206767
	speed: 2.4269s/iter; left time: 11639.1817s
	iters: 1000, epoch: 6 | loss: 0.2609707
	speed: 2.5049s/iter; left time: 11762.8724s
	iters: 1100, epoch: 6 | loss: 0.3132351
	speed: 2.5173s/iter; left time: 11569.3352s
Epoch: 6 cost time: 2829.4110662937164
Epoch: 6, Steps: 1139 | Train Loss: 0.2513304 Vali Loss: 0.1591964 Test Loss: 0.1795650
Validation loss decreased (0.160054 --> 0.159196).  Saving model ...
Updating learning rate to 3.125e-06
	iters: 100, epoch: 7 | loss: 0.2743181
	speed: 6.3418s/iter; left time: 28265.2224s
	iters: 200, epoch: 7 | loss: 0.2289606
	speed: 2.5177s/iter; left time: 10969.7575s
	iters: 300, epoch: 7 | loss: 0.2524424
	speed: 2.5177s/iter; left time: 10717.6724s
	iters: 400, epoch: 7 | loss: 0.2807781
	speed: 2.5177s/iter; left time: 10466.0860s
	iters: 500, epoch: 7 | loss: 0.2501821
	speed: 2.5179s/iter; left time: 10215.2436s
	iters: 600, epoch: 7 | loss: 0.2458116
	speed: 2.5176s/iter; left time: 9962.0714s
	iters: 700, epoch: 7 | loss: 0.2544238
	speed: 2.5185s/iter; left time: 9713.6831s
	iters: 800, epoch: 7 | loss: 0.2414929
	speed: 2.5177s/iter; left time: 9458.9278s
	iters: 900, epoch: 7 | loss: 0.2301904
	speed: 2.5184s/iter; left time: 9209.6394s
	iters: 1000, epoch: 7 | loss: 0.2485033
	speed: 2.5176s/iter; left time: 8955.1771s
	iters: 1100, epoch: 7 | loss: 0.2292460
	speed: 2.5185s/iter; left time: 8706.3034s
Epoch: 7 cost time: 2866.436535835266
Epoch: 7, Steps: 1139 | Train Loss: 0.2509177 Vali Loss: 0.1592148 Test Loss: 0.1798812
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
	iters: 100, epoch: 8 | loss: 0.2596500
	speed: 6.3468s/iter; left time: 21058.8392s
	iters: 200, epoch: 8 | loss: 0.2568321
	speed: 2.5184s/iter; left time: 8104.2688s
	iters: 300, epoch: 8 | loss: 0.2173257
	speed: 2.5172s/iter; left time: 7848.5214s
	iters: 400, epoch: 8 | loss: 0.2715625
	speed: 2.5178s/iter; left time: 7598.8580s
	iters: 500, epoch: 8 | loss: 0.2481260
	speed: 2.5177s/iter; left time: 7346.6758s
	iters: 600, epoch: 8 | loss: 0.2558088
	speed: 2.5166s/iter; left time: 7091.7713s
	iters: 700, epoch: 8 | loss: 0.2748639
	speed: 2.5172s/iter; left time: 6841.6430s
	iters: 800, epoch: 8 | loss: 0.2455639
	speed: 2.5180s/iter; left time: 6592.1858s
	iters: 900, epoch: 8 | loss: 0.2490485
	speed: 2.5179s/iter; left time: 6339.9575s
	iters: 1000, epoch: 8 | loss: 0.2502836
	speed: 2.5173s/iter; left time: 6086.7570s
	iters: 1100, epoch: 8 | loss: 0.2586440
	speed: 2.3738s/iter; left time: 5502.5354s
Epoch: 8 cost time: 2845.0779044628143
Epoch: 8, Steps: 1139 | Train Loss: 0.2506972 Vali Loss: 0.1592519 Test Loss: 0.1800570
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
	iters: 100, epoch: 9 | loss: 0.2419635
	speed: 6.0134s/iter; left time: 13103.1954s
	iters: 200, epoch: 9 | loss: 0.2413998
	speed: 2.5172s/iter; left time: 5233.3451s
	iters: 300, epoch: 9 | loss: 0.2600816
	speed: 2.5182s/iter; left time: 4983.5034s
	iters: 400, epoch: 9 | loss: 0.2716661
	speed: 2.5153s/iter; left time: 4726.2101s
	iters: 500, epoch: 9 | loss: 0.2507571
	speed: 2.5173s/iter; left time: 4478.3276s
	iters: 600, epoch: 9 | loss: 0.2466055
	speed: 2.5131s/iter; left time: 4219.5498s
	iters: 700, epoch: 9 | loss: 0.2875139
	speed: 2.5185s/iter; left time: 3976.6636s
	iters: 800, epoch: 9 | loss: 0.2239460
	speed: 2.5143s/iter; left time: 3718.6581s
	iters: 900, epoch: 9 | loss: 0.2707089
	speed: 2.5174s/iter; left time: 3471.5258s
	iters: 1000, epoch: 9 | loss: 0.2395290
	speed: 2.5141s/iter; left time: 3215.4936s
	iters: 1100, epoch: 9 | loss: 0.2619624
	speed: 2.5123s/iter; left time: 2961.9837s
Epoch: 9 cost time: 2853.8533957004547
Epoch: 9, Steps: 1139 | Train Loss: 0.2506459 Vali Loss: 0.1592093 Test Loss: 0.1795277
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_ECL_96_96_AGPT_PT_2048_fixedFalse_0.0001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5165
test shape: (5165, 96, 321) (5165, 96, 321)
test shape: (5165, 96, 321) (5165, 96, 321)
mse:0.17954635620117188, mae:0.26669958233833313
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_192          Model:              AGPT_PT             

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_192_AGPT_PT_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 18125
val 2441
test 5069
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 337, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 268, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 70, in forward
    V = torch.einsum("bhls,bshd->blhd", A, values)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/functional.py", line 386, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 10.12 MiB is free. Process 2582933 has 30.04 GiB memory in use. Process 3949244 has 49.08 GiB memory in use. Of the allocated memory 48.40 GiB is allocated by PyTorch, and 196.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_336          Model:              AGPT_PT             

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_336_AGPT_PT_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17981
val 2297
test 4925
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 337, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 268, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 69, in forward
    A = self.dropout(torch.softmax(scale * scores, dim=-1))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 6.12 MiB is free. Process 2582933 has 30.04 GiB memory in use. Process 3950384 has 49.09 GiB memory in use. Of the allocated memory 48.37 GiB is allocated by PyTorch, and 235.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           ECL_96_720          Model:              AGPT_PT             

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/electricity/
  Data Path:          electricity.csv     Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             321                 Dec In:             321                 
  C Out:              321                 d model:            512                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_ECL_96_720_AGPT_PT_2048_fixedFalse_0.0001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 17597
val 1913
test 4541
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 337, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_PT.py", line 268, in forecast
    segment_out, _ = self.encoder(segment_input)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 69, in forward
    A = self.dropout(torch.softmax(scale * scores, dim=-1))
                                   ~~~~~~^~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 36.12 MiB is free. Process 2582933 has 30.04 GiB memory in use. Process 3951788 has 49.06 GiB memory in use. Of the allocated memory 48.36 GiB is allocated by PyTorch, and 216.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
