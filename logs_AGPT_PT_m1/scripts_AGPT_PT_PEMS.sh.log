Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS03              Model:              AGPT_PT             

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS03.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             358                 Dec In:             358                 
  C Out:              358                 d model:            128                 
  n heads:            8                   e layers:           1                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      1                   Devices:            0,1                 

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS03_AGPT_PT_256_fixedFalse_0.003_0.001_CD>>>>>>>>>>>>>>>>>>>>>>>>>>
train 15617
val 5135
test 5135
	iters: 100, epoch: 1 | loss: 0.2003520
	speed: 1.1461s/iter; left time: 5490.8528s
	iters: 200, epoch: 1 | loss: 0.1875549
	speed: 1.1002s/iter; left time: 5161.0696s
	iters: 300, epoch: 1 | loss: 0.2370630
	speed: 1.0105s/iter; left time: 4639.3098s
	iters: 400, epoch: 1 | loss: 0.2059378
	speed: 1.0786s/iter; left time: 4843.8026s
Epoch: 1 cost time: 533.8027501106262
Epoch: 1, Steps: 489 | Train Loss: 0.2221529 Vali Loss: 0.0941393 Test Loss: 0.0949814
Validation loss decreased (inf --> 0.094139).  Saving model ...
Updating learning rate to 0.006
	iters: 100, epoch: 2 | loss: 0.2631555
	speed: 6.4574s/iter; left time: 27779.6859s
	iters: 200, epoch: 2 | loss: 0.2556442
	speed: 1.1196s/iter; left time: 4704.6699s
	iters: 300, epoch: 2 | loss: 0.2363224
	speed: 1.0806s/iter; left time: 4432.5947s
	iters: 400, epoch: 2 | loss: 0.1639674
	speed: 1.0441s/iter; left time: 4178.5175s
Epoch: 2 cost time: 519.9133355617523
Traceback (most recent call last):
  File "/root/daye/AGPT/run.py", line 224, in <module>
    exp.train(setting)
  File "/root/daye/AGPT/exp/exp_AGPT.py", line 187, in train
    vali_loss = self.vali(vali_data, vali_loader, criterion)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/daye/AGPT/exp/exp_AGPT.py", line 66, in vali
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py", line 185, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py", line 200, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/Time/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py", line 100, in parallel_apply
    thread.join()
  File "/root/anaconda3/envs/Time/lib/python3.11/threading.py", line 1119, in join
    self._wait_for_tstate_lock()
  File "/root/anaconda3/envs/Time/lib/python3.11/threading.py", line 1139, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
