Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS03              Model:              AGPT_loss           

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS03.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             358                 Dec In:             358                 
  C Out:              358                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS03_AGPT_loss_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 15521
val 5039
test 5039
	iters: 100, epoch: 1 | loss: 0.2903678
	speed: 0.9243s/iter; left time: 4400.6786s
	iters: 200, epoch: 1 | loss: 0.2318158
	speed: 0.9597s/iter; left time: 4473.0008s
	iters: 300, epoch: 1 | loss: 0.2484361
	speed: 0.9950s/iter; left time: 4538.2571s
	iters: 400, epoch: 1 | loss: 0.2565118
	speed: 0.9741s/iter; left time: 4345.3082s
Epoch: 1 cost time: 469.37961196899414
Epoch: 1, Steps: 486 | Train Loss: 0.3085668 Vali Loss: 0.0985879 Test Loss: 0.0952097
Validation loss decreased (inf --> 0.098588).  Saving model ...
Updating learning rate to 0.003
	iters: 100, epoch: 2 | loss: 0.2184153
	speed: 3.2398s/iter; left time: 13850.1793s
	iters: 200, epoch: 2 | loss: 0.2244783
	speed: 0.9636s/iter; left time: 4022.8786s
	iters: 300, epoch: 2 | loss: 0.2207215
	speed: 0.9078s/iter; left time: 3699.4446s
	iters: 400, epoch: 2 | loss: 0.2682400
	speed: 0.8759s/iter; left time: 3481.6654s
Epoch: 2 cost time: 451.2745807170868
Epoch: 2, Steps: 486 | Train Loss: 0.2340368 Vali Loss: 0.1138473 Test Loss: 0.1099164
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0015
	iters: 100, epoch: 3 | loss: 0.2731211
	speed: 3.0298s/iter; left time: 11479.8751s
	iters: 200, epoch: 3 | loss: 0.2381699
	speed: 0.9604s/iter; left time: 3542.8853s
	iters: 300, epoch: 3 | loss: 0.2303219
	speed: 0.9866s/iter; left time: 3540.7501s
	iters: 400, epoch: 3 | loss: 0.2276721
	speed: 0.9990s/iter; left time: 3485.5735s
Epoch: 3 cost time: 465.54579734802246
Epoch: 3, Steps: 486 | Train Loss: 0.2272016 Vali Loss: 0.0865719 Test Loss: 0.0869872
Validation loss decreased (0.098588 --> 0.086572).  Saving model ...
Updating learning rate to 0.00075
	iters: 100, epoch: 4 | loss: 0.2322977
	speed: 2.9623s/iter; left time: 9784.4042s
	iters: 200, epoch: 4 | loss: 0.2346708
	speed: 0.8891s/iter; left time: 2847.8645s
	iters: 300, epoch: 4 | loss: 0.1899861
	speed: 0.9875s/iter; left time: 3064.0957s
	iters: 400, epoch: 4 | loss: 0.2109325
	speed: 0.9755s/iter; left time: 2929.5686s
Epoch: 4 cost time: 437.8720452785492
Epoch: 4, Steps: 486 | Train Loss: 0.2223200 Vali Loss: 0.0766575 Test Loss: 0.0762308
Validation loss decreased (0.086572 --> 0.076658).  Saving model ...
Updating learning rate to 0.000375
	iters: 100, epoch: 5 | loss: 0.2475839
	speed: 2.8613s/iter; left time: 8060.2245s
	iters: 200, epoch: 5 | loss: 0.2032901
	speed: 0.9865s/iter; left time: 2680.2466s
	iters: 300, epoch: 5 | loss: 0.2157550
	speed: 0.9309s/iter; left time: 2436.2674s
	iters: 400, epoch: 5 | loss: 0.2450938
	speed: 0.8245s/iter; left time: 2075.2729s
Epoch: 5 cost time: 433.0309658050537
Epoch: 5, Steps: 486 | Train Loss: 0.2198729 Vali Loss: 0.0760975 Test Loss: 0.0760279
Validation loss decreased (0.076658 --> 0.076097).  Saving model ...
Updating learning rate to 0.0001875
	iters: 100, epoch: 6 | loss: 0.2151645
	speed: 3.5543s/iter; left time: 8285.1077s
	iters: 200, epoch: 6 | loss: 0.1852706
	speed: 0.9044s/iter; left time: 2017.6492s
	iters: 300, epoch: 6 | loss: 0.2323595
	speed: 0.7960s/iter; left time: 1696.2640s
	iters: 400, epoch: 6 | loss: 0.2405806
	speed: 0.9644s/iter; left time: 1958.6360s
Epoch: 6 cost time: 456.9108510017395
Epoch: 6, Steps: 486 | Train Loss: 0.2184337 Vali Loss: 0.0757028 Test Loss: 0.0753556
Validation loss decreased (0.076097 --> 0.075703).  Saving model ...
Updating learning rate to 9.375e-05
	iters: 100, epoch: 7 | loss: 0.1974446
	speed: 3.2516s/iter; left time: 5999.2661s
	iters: 200, epoch: 7 | loss: 0.2269933
	speed: 1.0298s/iter; left time: 1796.9822s
	iters: 300, epoch: 7 | loss: 0.2040541
	speed: 0.8955s/iter; left time: 1473.0170s
	iters: 400, epoch: 7 | loss: 0.2147933
	speed: 0.8828s/iter; left time: 1363.9959s
Epoch: 7 cost time: 477.79892349243164
Epoch: 7, Steps: 486 | Train Loss: 0.2175083 Vali Loss: 0.0762428 Test Loss: 0.0762214
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.6875e-05
	iters: 100, epoch: 8 | loss: 0.2009380
	speed: 3.1355s/iter; left time: 4261.1632s
	iters: 200, epoch: 8 | loss: 0.2110420
	speed: 0.9813s/iter; left time: 1235.4732s
	iters: 300, epoch: 8 | loss: 0.1987396
	speed: 0.8133s/iter; left time: 942.6180s
	iters: 400, epoch: 8 | loss: 0.2415407
	speed: 0.7722s/iter; left time: 817.7795s
Epoch: 8 cost time: 438.3777072429657
Epoch: 8, Steps: 486 | Train Loss: 0.2166470 Vali Loss: 0.0729909 Test Loss: 0.0732627
Validation loss decreased (0.075703 --> 0.072991).  Saving model ...
Updating learning rate to 2.34375e-05
	iters: 100, epoch: 9 | loss: 0.2269048
	speed: 3.1673s/iter; left time: 2765.0933s
	iters: 200, epoch: 9 | loss: 0.2225841
	speed: 1.0116s/iter; left time: 781.9591s
	iters: 300, epoch: 9 | loss: 0.2126322
	speed: 1.1332s/iter; left time: 762.6334s
	iters: 400, epoch: 9 | loss: 0.2006714
	speed: 1.0409s/iter; left time: 596.4610s
Epoch: 9 cost time: 509.77023005485535
Epoch: 9, Steps: 486 | Train Loss: 0.2167656 Vali Loss: 0.0741232 Test Loss: 0.0744513
EarlyStopping counter: 1 out of 10
Updating learning rate to 1.171875e-05
	iters: 100, epoch: 10 | loss: 0.1965243
	speed: 3.2637s/iter; left time: 1263.0438s
	iters: 200, epoch: 10 | loss: 0.1946076
	speed: 1.1331s/iter; left time: 325.1983s
	iters: 300, epoch: 10 | loss: 0.2049253
	speed: 0.9960s/iter; left time: 186.2468s
	iters: 400, epoch: 10 | loss: 0.1842513
	speed: 0.9450s/iter; left time: 82.2144s
Epoch: 10 cost time: 490.4663052558899
Epoch: 10, Steps: 486 | Train Loss: 0.2166688 Vali Loss: 0.0731886 Test Loss: 0.0734458
EarlyStopping counter: 2 out of 10
Updating learning rate to 5.859375e-06
>>>>>>>testing : AGPT_loss_PEMS03_AGPT_loss_256_fixedFalse_0.003_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 5039
test shape: (5039, 12, 358) (5039, 12, 358)
test shape: (5039, 12, 358) (5039, 12, 358)
mse:0.07329970598220825, mae:0.18313062191009521
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS04              Model:              AGPT_loss           

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS04.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             307                 Dec In:             307                 
  C Out:              307                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS04_AGPT_loss_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 9992
val 3195
test 3196
	iters: 100, epoch: 1 | loss: 0.2896564
	speed: 0.9626s/iter; left time: 2917.6826s
	iters: 200, epoch: 1 | loss: 0.2496036
	speed: 0.9900s/iter; left time: 2901.6090s
	iters: 300, epoch: 1 | loss: 0.2737716
	speed: 0.8615s/iter; left time: 2439.0052s
Epoch: 1 cost time: 292.17147397994995
Epoch: 1, Steps: 313 | Train Loss: 0.3914164 Vali Loss: 0.1153827 Test Loss: 0.1073079
Validation loss decreased (inf --> 0.115383).  Saving model ...
Updating learning rate to 0.003
	iters: 100, epoch: 2 | loss: 0.2583965
	speed: 1.6936s/iter; left time: 4603.2707s
	iters: 200, epoch: 2 | loss: 0.2382076
	speed: 0.8850s/iter; left time: 2316.8997s
	iters: 300, epoch: 2 | loss: 0.2532610
	speed: 0.8629s/iter; left time: 2172.7180s
Epoch: 2 cost time: 274.8614730834961
Epoch: 2, Steps: 313 | Train Loss: 0.2445329 Vali Loss: 0.1257868 Test Loss: 0.1162269
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.0015
	iters: 100, epoch: 3 | loss: 0.2339820
	speed: 1.4191s/iter; left time: 3412.8399s
	iters: 200, epoch: 3 | loss: 0.2403572
	speed: 0.6202s/iter; left time: 1429.5618s
	iters: 300, epoch: 3 | loss: 0.2455447
	speed: 0.8774s/iter; left time: 1934.7601s
Epoch: 3 cost time: 224.3772566318512
Epoch: 3, Steps: 313 | Train Loss: 0.2350888 Vali Loss: 0.1067984 Test Loss: 0.1004770
Validation loss decreased (0.115383 --> 0.106798).  Saving model ...
Updating learning rate to 0.00075
	iters: 100, epoch: 4 | loss: 0.2001403
	speed: 1.6564s/iter; left time: 3465.1510s
	iters: 200, epoch: 4 | loss: 0.2087365
	speed: 0.8635s/iter; left time: 1720.0162s
	iters: 300, epoch: 4 | loss: 0.2439914
	speed: 0.8350s/iter; left time: 1579.8806s
Epoch: 4 cost time: 266.63367986679077
Epoch: 4, Steps: 313 | Train Loss: 0.2319155 Vali Loss: 0.1036667 Test Loss: 0.0977942
Validation loss decreased (0.106798 --> 0.103667).  Saving model ...
Updating learning rate to 0.000375
	iters: 100, epoch: 5 | loss: 0.2395001
	speed: 1.6340s/iter; left time: 2906.8891s
	iters: 200, epoch: 5 | loss: 0.2285219
	speed: 0.8637s/iter; left time: 1450.0823s
	iters: 300, epoch: 5 | loss: 0.2123559
	speed: 0.8835s/iter; left time: 1395.0603s
Epoch: 5 cost time: 272.0637676715851
Epoch: 5, Steps: 313 | Train Loss: 0.2302398 Vali Loss: 0.1002230 Test Loss: 0.0943885
Validation loss decreased (0.103667 --> 0.100223).  Saving model ...
Updating learning rate to 0.0001875
	iters: 100, epoch: 6 | loss: 0.2490786
	speed: 1.5198s/iter; left time: 2228.0247s
	iters: 200, epoch: 6 | loss: 0.2292300
	speed: 0.7875s/iter; left time: 1075.7824s
	iters: 300, epoch: 6 | loss: 0.2401260
	speed: 0.7650s/iter; left time: 968.4378s
Epoch: 6 cost time: 236.66191053390503
Epoch: 6, Steps: 313 | Train Loss: 0.2291651 Vali Loss: 0.1005450 Test Loss: 0.0946648
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.375e-05
	iters: 100, epoch: 7 | loss: 0.2362404
	speed: 1.6520s/iter; left time: 1904.7190s
	iters: 200, epoch: 7 | loss: 0.2510740
	speed: 0.8567s/iter; left time: 902.1439s
	iters: 300, epoch: 7 | loss: 0.2391138
	speed: 0.8085s/iter; left time: 770.4612s
Epoch: 7 cost time: 265.2789628505707
Epoch: 7, Steps: 313 | Train Loss: 0.2286728 Vali Loss: 0.1003000 Test Loss: 0.0947431
EarlyStopping counter: 2 out of 10
Updating learning rate to 4.6875e-05
	iters: 100, epoch: 8 | loss: 0.2334642
	speed: 1.6204s/iter; left time: 1361.1106s
	iters: 200, epoch: 8 | loss: 0.2618696
	speed: 0.9597s/iter; left time: 710.1935s
	iters: 300, epoch: 8 | loss: 0.2598714
	speed: 0.8974s/iter; left time: 574.3224s
Epoch: 8 cost time: 285.65761613845825
Epoch: 8, Steps: 313 | Train Loss: 0.2281927 Vali Loss: 0.1009586 Test Loss: 0.0952885
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.34375e-05
	iters: 100, epoch: 9 | loss: 0.2553209
	speed: 1.6783s/iter; left time: 884.4522s
	iters: 200, epoch: 9 | loss: 0.2213922
	speed: 0.9000s/iter; left time: 384.3208s
	iters: 300, epoch: 9 | loss: 0.2157499
	speed: 0.9573s/iter; left time: 313.0487s
Epoch: 9 cost time: 280.77047061920166
Epoch: 9, Steps: 313 | Train Loss: 0.2282643 Vali Loss: 0.1005278 Test Loss: 0.0947146
EarlyStopping counter: 4 out of 10
Updating learning rate to 1.171875e-05
	iters: 100, epoch: 10 | loss: 0.2374391
	speed: 1.6045s/iter; left time: 343.3583s
	iters: 200, epoch: 10 | loss: 0.2313797
	speed: 0.9213s/iter; left time: 105.0308s
	iters: 300, epoch: 10 | loss: 0.2299161
	speed: 0.9292s/iter; left time: 13.0089s
Epoch: 10 cost time: 283.3872151374817
Epoch: 10, Steps: 313 | Train Loss: 0.2281392 Vali Loss: 0.1010689 Test Loss: 0.0952347
EarlyStopping counter: 5 out of 10
Updating learning rate to 5.859375e-06
>>>>>>>testing : AGPT_loss_PEMS04_AGPT_loss_256_fixedFalse_0.003_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3196
test shape: (3196, 12, 307) (3196, 12, 307)
test shape: (3196, 12, 307) (3196, 12, 307)
mse:0.09443871676921844, mae:0.20857135951519012
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS07              Model:              AGPT_loss           

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS07.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             883                 Dec In:             883                 
  C Out:              883                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS07_AGPT_loss_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 16731
val 5442
test 5442
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss.py", line 214, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss.py", line 194, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 69, in forward
    A = self.dropout(torch.softmax(scale * scores, dim=-1))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 289.44 MiB is free. Process 3624889 has 25.61 GiB memory in use. Process 3624935 has 23.79 GiB memory in use. Process 847295 has 1.88 GiB memory in use. Process 1183236 has 7.92 GiB memory in use. Process 1330431 has 19.64 GiB memory in use. Of the allocated memory 19.04 GiB is allocated by PyTorch, and 111.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           PEMS08              Model:              AGPT_loss           

[1mData Loader[0m
  Data:               PEMS                Root Path:          ./dataset/PEMS/     
  Data Path:          PEMS08.npz          Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             170                 Dec In:             170                 
  C Out:              170                 d model:            128                 
  n heads:            8                   e layers:           5                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           10                  Learning Rate:      0.003               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_PEMS08_AGPT_loss_256_fixedFalse_0.003_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 10510
val 3368
test 3369
	iters: 100, epoch: 1 | loss: 0.2985766
	speed: 0.5791s/iter; left time: 1847.7734s
	iters: 200, epoch: 1 | loss: 0.2657421
	speed: 0.5820s/iter; left time: 1799.0919s
	iters: 300, epoch: 1 | loss: 0.2412111
	speed: 0.5544s/iter; left time: 1658.1323s
Epoch: 1 cost time: 184.27714824676514
Epoch: 1, Steps: 329 | Train Loss: 0.4045037 Vali Loss: 0.1345511 Test Loss: 0.1213990
Validation loss decreased (inf --> 0.134551).  Saving model ...
Updating learning rate to 0.003
	iters: 100, epoch: 2 | loss: 0.2192683
	speed: 1.1790s/iter; left time: 3374.2409s
	iters: 200, epoch: 2 | loss: 0.2537135
	speed: 0.5522s/iter; left time: 1525.1892s
	iters: 300, epoch: 2 | loss: 0.2470973
	speed: 0.5812s/iter; left time: 1547.1768s
Epoch: 2 cost time: 184.09010529518127
Epoch: 2, Steps: 329 | Train Loss: 0.2425803 Vali Loss: 0.1100787 Test Loss: 0.0968804
Validation loss decreased (0.134551 --> 0.110079).  Saving model ...
Updating learning rate to 0.0015
	iters: 100, epoch: 3 | loss: 0.2591335
	speed: 1.4794s/iter; left time: 3747.3778s
	iters: 200, epoch: 3 | loss: 0.2207426
	speed: 0.6291s/iter; left time: 1530.6169s
	iters: 300, epoch: 3 | loss: 0.2245089
	speed: 0.6030s/iter; left time: 1406.8526s
Epoch: 3 cost time: 202.1191041469574
Epoch: 3, Steps: 329 | Train Loss: 0.2330607 Vali Loss: 0.1115754 Test Loss: 0.0979592
EarlyStopping counter: 1 out of 10
Updating learning rate to 0.00075
	iters: 100, epoch: 4 | loss: 0.2195863
	speed: 1.2225s/iter; left time: 2694.3676s
	iters: 200, epoch: 4 | loss: 0.2379040
	speed: 0.5771s/iter; left time: 1214.1892s
	iters: 300, epoch: 4 | loss: 0.2402866
	speed: 0.5564s/iter; left time: 1114.9428s
Epoch: 4 cost time: 184.76490139961243
Epoch: 4, Steps: 329 | Train Loss: 0.2301643 Vali Loss: 0.1059859 Test Loss: 0.0937338
Validation loss decreased (0.110079 --> 0.105986).  Saving model ...
Updating learning rate to 0.000375
	iters: 100, epoch: 5 | loss: 0.2475565
	speed: 1.4058s/iter; left time: 2635.8842s
	iters: 200, epoch: 5 | loss: 0.2217908
	speed: 0.5873s/iter; left time: 1042.4455s
	iters: 300, epoch: 5 | loss: 0.2481963
	speed: 0.6184s/iter; left time: 1035.8079s
Epoch: 5 cost time: 201.66417837142944
Epoch: 5, Steps: 329 | Train Loss: 0.2277874 Vali Loss: 0.1022813 Test Loss: 0.0905682
Validation loss decreased (0.105986 --> 0.102281).  Saving model ...
Updating learning rate to 0.0001875
	iters: 100, epoch: 6 | loss: 0.2302586
	speed: 1.3029s/iter; left time: 2014.2460s
	iters: 200, epoch: 6 | loss: 0.2432454
	speed: 0.5495s/iter; left time: 794.6180s
	iters: 300, epoch: 6 | loss: 0.2297166
	speed: 0.6068s/iter; left time: 816.7648s
Epoch: 6 cost time: 186.98049473762512
Epoch: 6, Steps: 329 | Train Loss: 0.2267218 Vali Loss: 0.1043598 Test Loss: 0.0924170
EarlyStopping counter: 1 out of 10
Updating learning rate to 9.375e-05
	iters: 100, epoch: 7 | loss: 0.2369993
	speed: 1.2882s/iter; left time: 1567.6906s
	iters: 200, epoch: 7 | loss: 0.2265661
	speed: 0.5929s/iter; left time: 662.2727s
	iters: 300, epoch: 7 | loss: 0.2187213
	speed: 0.6299s/iter; left time: 640.6276s
Epoch: 7 cost time: 195.03629970550537
Epoch: 7, Steps: 329 | Train Loss: 0.2257626 Vali Loss: 0.1016958 Test Loss: 0.0898171
Validation loss decreased (0.102281 --> 0.101696).  Saving model ...
Updating learning rate to 4.6875e-05
	iters: 100, epoch: 8 | loss: 0.2397926
	speed: 1.3582s/iter; left time: 1206.0945s
	iters: 200, epoch: 8 | loss: 0.2193763
	speed: 0.6318s/iter; left time: 497.8845s
	iters: 300, epoch: 8 | loss: 0.2153807
	speed: 0.6036s/iter; left time: 415.2637s
Epoch: 8 cost time: 199.93538093566895
Epoch: 8, Steps: 329 | Train Loss: 0.2254613 Vali Loss: 0.1032200 Test Loss: 0.0914763
EarlyStopping counter: 1 out of 10
Updating learning rate to 2.34375e-05
	iters: 100, epoch: 9 | loss: 0.2008717
	speed: 1.2125s/iter; left time: 677.7681s
	iters: 200, epoch: 9 | loss: 0.2243168
	speed: 0.5416s/iter; left time: 248.5848s
	iters: 300, epoch: 9 | loss: 0.2308944
	speed: 0.4412s/iter; left time: 158.4053s
Epoch: 9 cost time: 168.2083649635315
Epoch: 9, Steps: 329 | Train Loss: 0.2253720 Vali Loss: 0.1032854 Test Loss: 0.0914899
EarlyStopping counter: 2 out of 10
Updating learning rate to 1.171875e-05
	iters: 100, epoch: 10 | loss: 0.2421395
	speed: 1.2100s/iter; left time: 278.2911s
	iters: 200, epoch: 10 | loss: 0.2321764
	speed: 0.5998s/iter; left time: 77.9722s
	iters: 300, epoch: 10 | loss: 0.2313443
	speed: 0.5839s/iter; left time: 17.5181s
Epoch: 10 cost time: 189.35999870300293
Epoch: 10, Steps: 329 | Train Loss: 0.2251694 Vali Loss: 0.1031886 Test Loss: 0.0913072
EarlyStopping counter: 3 out of 10
Updating learning rate to 5.859375e-06
>>>>>>>testing : AGPT_loss_PEMS08_AGPT_loss_256_fixedFalse_0.003_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3369
test shape: (3369, 12, 170) (3369, 12, 170)
test shape: (3369, 12, 170) (3369, 12, 170)
mse:0.090057872235775, mae:0.1983356773853302
