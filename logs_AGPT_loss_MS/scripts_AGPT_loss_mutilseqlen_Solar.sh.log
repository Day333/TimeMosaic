Running Solar with seq_len=96, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_96_96         Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_96_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36601
val 5161
test 10417
	iters: 100, epoch: 1 | loss: 0.4856168
	speed: 0.4880s/iter; left time: 5534.5913s
	iters: 200, epoch: 1 | loss: 0.3466048
	speed: 0.4843s/iter; left time: 5444.2625s
	iters: 300, epoch: 1 | loss: 0.3571521
	speed: 0.4834s/iter; left time: 5385.5799s
	iters: 400, epoch: 1 | loss: 0.3119499
	speed: 0.4840s/iter; left time: 5344.2371s
	iters: 500, epoch: 1 | loss: 0.4422815
	speed: 0.4833s/iter; left time: 5288.1831s
	iters: 600, epoch: 1 | loss: 0.3314037
	speed: 0.4853s/iter; left time: 5260.6269s
	iters: 700, epoch: 1 | loss: 0.3664535
	speed: 0.4861s/iter; left time: 5221.6218s
	iters: 800, epoch: 1 | loss: 0.2824312
	speed: 0.4845s/iter; left time: 5155.3770s
	iters: 900, epoch: 1 | loss: 0.3167000
	speed: 0.4770s/iter; left time: 5027.6809s
	iters: 1000, epoch: 1 | loss: 0.3211772
	speed: 0.4053s/iter; left time: 4232.2266s
	iters: 1100, epoch: 1 | loss: 0.3758556
	speed: 0.4050s/iter; left time: 4187.8634s
Epoch: 1 cost time: 534.371454000473
Epoch: 1, Steps: 1144 | Train Loss: 0.3824507 Vali Loss: 0.6179981 Test Loss: 0.5506295
Validation loss decreased (inf --> 0.617998).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.3303473
	speed: 1.2270s/iter; left time: 12511.3986s
	iters: 200, epoch: 2 | loss: 0.4236799
	speed: 0.4837s/iter; left time: 4884.0369s
	iters: 300, epoch: 2 | loss: 0.2752032
	speed: 0.4844s/iter; left time: 4842.7530s
	iters: 400, epoch: 2 | loss: 0.3609167
	speed: 0.4855s/iter; left time: 4804.9303s
	iters: 500, epoch: 2 | loss: 0.2920514
	speed: 0.4862s/iter; left time: 4763.4951s
	iters: 600, epoch: 2 | loss: 0.3270980
	speed: 0.4862s/iter; left time: 4714.8728s
	iters: 700, epoch: 2 | loss: 0.3784828
	speed: 0.4841s/iter; left time: 4645.4711s
	iters: 800, epoch: 2 | loss: 0.3719302
	speed: 0.4863s/iter; left time: 4618.5528s
	iters: 900, epoch: 2 | loss: 0.3354733
	speed: 0.4863s/iter; left time: 4569.9645s
	iters: 1000, epoch: 2 | loss: 0.3398553
	speed: 0.4866s/iter; left time: 4523.7030s
	iters: 1100, epoch: 2 | loss: 0.3949164
	speed: 0.4843s/iter; left time: 4454.5169s
Epoch: 2 cost time: 554.4602284431458
Epoch: 2, Steps: 1144 | Train Loss: 0.3405793 Vali Loss: 0.1903705 Test Loss: 0.2433546
Validation loss decreased (0.617998 --> 0.190371).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.2649805
	speed: 1.3701s/iter; left time: 12403.1654s
	iters: 200, epoch: 3 | loss: 0.3700413
	speed: 0.4849s/iter; left time: 4341.1826s
	iters: 300, epoch: 3 | loss: 0.3017024
	speed: 0.4858s/iter; left time: 4300.9738s
	iters: 400, epoch: 3 | loss: 0.2964023
	speed: 0.4862s/iter; left time: 4255.8837s
	iters: 500, epoch: 3 | loss: 0.3102025
	speed: 0.4837s/iter; left time: 4185.3685s
	iters: 600, epoch: 3 | loss: 0.2631501
	speed: 0.4834s/iter; left time: 4134.5459s
	iters: 700, epoch: 3 | loss: 0.3555701
	speed: 0.4842s/iter; left time: 4092.8633s
	iters: 800, epoch: 3 | loss: 0.3100768
	speed: 0.4844s/iter; left time: 4046.0088s
	iters: 900, epoch: 3 | loss: 0.3328633
	speed: 0.4850s/iter; left time: 4002.8392s
	iters: 1000, epoch: 3 | loss: 0.3592950
	speed: 0.4366s/iter; left time: 3559.3611s
	iters: 1100, epoch: 3 | loss: 0.3889281
	speed: 0.4019s/iter; left time: 3236.8734s
Epoch: 3 cost time: 538.1138904094696
Epoch: 3, Steps: 1144 | Train Loss: 0.3248255 Vali Loss: 0.1798250 Test Loss: 0.2324276
Validation loss decreased (0.190371 --> 0.179825).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.3635665
	speed: 1.2448s/iter; left time: 9845.0039s
	iters: 200, epoch: 4 | loss: 0.3212255
	speed: 0.4844s/iter; left time: 3782.4386s
	iters: 300, epoch: 4 | loss: 0.2801873
	speed: 0.4824s/iter; left time: 3718.5089s
	iters: 400, epoch: 4 | loss: 0.2939627
	speed: 0.4862s/iter; left time: 3699.4878s
	iters: 500, epoch: 4 | loss: 0.2788617
	speed: 0.4859s/iter; left time: 3648.8467s
	iters: 600, epoch: 4 | loss: 0.3760003
	speed: 0.4818s/iter; left time: 3569.6193s
	iters: 700, epoch: 4 | loss: 0.2876275
	speed: 0.4846s/iter; left time: 3541.6768s
	iters: 800, epoch: 4 | loss: 0.2548281
	speed: 0.4844s/iter; left time: 3492.3784s
	iters: 900, epoch: 4 | loss: 0.2884119
	speed: 0.4828s/iter; left time: 3432.0409s
	iters: 1000, epoch: 4 | loss: 0.2786506
	speed: 0.4861s/iter; left time: 3407.0350s
	iters: 1100, epoch: 4 | loss: 0.3256567
	speed: 0.4835s/iter; left time: 3340.7285s
Epoch: 4 cost time: 551.6296660900116
Epoch: 4, Steps: 1144 | Train Loss: 0.3170195 Vali Loss: 0.1829154 Test Loss: 0.2286129
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 0.2945531
	speed: 1.3538s/iter; left time: 9158.6590s
	iters: 200, epoch: 5 | loss: 0.2854540
	speed: 0.4846s/iter; left time: 3230.0445s
	iters: 300, epoch: 5 | loss: 0.2568805
	speed: 0.4837s/iter; left time: 3175.6131s
	iters: 400, epoch: 5 | loss: 0.3382846
	speed: 0.4852s/iter; left time: 3136.7076s
	iters: 500, epoch: 5 | loss: 0.3429402
	speed: 0.4867s/iter; left time: 3097.7601s
	iters: 600, epoch: 5 | loss: 0.2758133
	speed: 0.4870s/iter; left time: 3051.1374s
	iters: 700, epoch: 5 | loss: 0.2913193
	speed: 0.4871s/iter; left time: 3003.2073s
	iters: 800, epoch: 5 | loss: 0.2869710
	speed: 0.4843s/iter; left time: 2937.1185s
	iters: 900, epoch: 5 | loss: 0.2932119
	speed: 0.4861s/iter; left time: 2899.7054s
	iters: 1000, epoch: 5 | loss: 0.2577322
	speed: 0.4627s/iter; left time: 2713.5545s
	iters: 1100, epoch: 5 | loss: 0.3165650
	speed: 0.4001s/iter; left time: 2306.3552s
Epoch: 5 cost time: 540.6617364883423
Epoch: 5, Steps: 1144 | Train Loss: 0.3120098 Vali Loss: 0.1727805 Test Loss: 0.2157881
Validation loss decreased (0.179825 --> 0.172781).  Saving model ...
Updating learning rate to 6.25e-05
	iters: 100, epoch: 6 | loss: 0.2767067
	speed: 1.1952s/iter; left time: 6717.9683s
	iters: 200, epoch: 6 | loss: 0.3113736
	speed: 0.4850s/iter; left time: 2677.7301s
	iters: 300, epoch: 6 | loss: 0.3215009
	speed: 0.4822s/iter; left time: 2614.2028s
	iters: 400, epoch: 6 | loss: 0.2568362
	speed: 0.4848s/iter; left time: 2579.4323s
	iters: 500, epoch: 6 | loss: 0.2821635
	speed: 0.4866s/iter; left time: 2540.4983s
	iters: 600, epoch: 6 | loss: 0.3470186
	speed: 0.4833s/iter; left time: 2475.1619s
	iters: 700, epoch: 6 | loss: 0.3510028
	speed: 0.4839s/iter; left time: 2429.7791s
	iters: 800, epoch: 6 | loss: 0.2428007
	speed: 0.4829s/iter; left time: 2376.2203s
	iters: 900, epoch: 6 | loss: 0.2638988
	speed: 0.4850s/iter; left time: 2338.3423s
	iters: 1000, epoch: 6 | loss: 0.4309610
	speed: 0.4862s/iter; left time: 2295.3058s
	iters: 1100, epoch: 6 | loss: 0.3495270
	speed: 0.4830s/iter; left time: 2232.1234s
Epoch: 6 cost time: 548.6013579368591
Epoch: 6, Steps: 1144 | Train Loss: 0.3089867 Vali Loss: 0.1696676 Test Loss: 0.2141343
Validation loss decreased (0.172781 --> 0.169668).  Saving model ...
Updating learning rate to 3.125e-05
	iters: 100, epoch: 7 | loss: 0.2780593
	speed: 1.3467s/iter; left time: 6029.0049s
	iters: 200, epoch: 7 | loss: 0.3232778
	speed: 0.4849s/iter; left time: 2122.2401s
	iters: 300, epoch: 7 | loss: 0.2783805
	speed: 0.4837s/iter; left time: 2068.9278s
	iters: 400, epoch: 7 | loss: 0.2574005
	speed: 0.4837s/iter; left time: 2020.3819s
	iters: 500, epoch: 7 | loss: 0.3667835
	speed: 0.4856s/iter; left time: 1979.8383s
	iters: 600, epoch: 7 | loss: 0.2931577
	speed: 0.4837s/iter; left time: 1923.6861s
	iters: 700, epoch: 7 | loss: 0.2556404
	speed: 0.4834s/iter; left time: 1874.0512s
	iters: 800, epoch: 7 | loss: 0.3632847
	speed: 0.4852s/iter; left time: 1832.7470s
	iters: 900, epoch: 7 | loss: 0.3675746
	speed: 0.4841s/iter; left time: 1779.9506s
	iters: 1000, epoch: 7 | loss: 0.3125553
	speed: 0.4844s/iter; left time: 1732.6946s
	iters: 1100, epoch: 7 | loss: 0.3930294
	speed: 0.4285s/iter; left time: 1489.9077s
Epoch: 7 cost time: 545.2231776714325
Epoch: 7, Steps: 1144 | Train Loss: 0.3070322 Vali Loss: 0.1694359 Test Loss: 0.2089362
Validation loss decreased (0.169668 --> 0.169436).  Saving model ...
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 8 | loss: 0.3458394
	speed: 1.2121s/iter; left time: 4039.8231s
	iters: 200, epoch: 8 | loss: 0.3723759
	speed: 0.4729s/iter; left time: 1529.0040s
	iters: 300, epoch: 8 | loss: 0.3146524
	speed: 0.4867s/iter; left time: 1524.7904s
	iters: 400, epoch: 8 | loss: 0.3115516
	speed: 0.4865s/iter; left time: 1475.6515s
	iters: 500, epoch: 8 | loss: 0.2945659
	speed: 0.4873s/iter; left time: 1429.1584s
	iters: 600, epoch: 8 | loss: 0.2937147
	speed: 0.4865s/iter; left time: 1378.1197s
	iters: 700, epoch: 8 | loss: 0.3089845
	speed: 0.4872s/iter; left time: 1331.4542s
	iters: 800, epoch: 8 | loss: 0.2551728
	speed: 0.4866s/iter; left time: 1281.3426s
	iters: 900, epoch: 8 | loss: 0.3170741
	speed: 0.4867s/iter; left time: 1232.6852s
	iters: 1000, epoch: 8 | loss: 0.2905937
	speed: 0.4872s/iter; left time: 1185.2529s
	iters: 1100, epoch: 8 | loss: 0.3082468
	speed: 0.4848s/iter; left time: 1131.0654s
Epoch: 8 cost time: 548.362167596817
Epoch: 8, Steps: 1144 | Train Loss: 0.3057761 Vali Loss: 0.1670565 Test Loss: 0.2079477
Validation loss decreased (0.169436 --> 0.167057).  Saving model ...
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 9 | loss: 0.2402283
	speed: 1.3803s/iter; left time: 3021.5075s
	iters: 200, epoch: 9 | loss: 0.3597514
	speed: 0.4830s/iter; left time: 1009.0536s
	iters: 300, epoch: 9 | loss: 0.2754579
	speed: 0.4870s/iter; left time: 968.6328s
	iters: 400, epoch: 9 | loss: 0.2309267
	speed: 0.4857s/iter; left time: 917.4334s
	iters: 500, epoch: 9 | loss: 0.3231070
	speed: 0.4831s/iter; left time: 864.2942s
	iters: 600, epoch: 9 | loss: 0.4472574
	speed: 0.4847s/iter; left time: 818.7349s
	iters: 700, epoch: 9 | loss: 0.3335606
	speed: 0.4823s/iter; left time: 766.3564s
	iters: 800, epoch: 9 | loss: 0.3685227
	speed: 0.4845s/iter; left time: 721.3515s
	iters: 900, epoch: 9 | loss: 0.3216011
	speed: 0.4838s/iter; left time: 671.9986s
	iters: 1000, epoch: 9 | loss: 0.3221782
	speed: 0.4867s/iter; left time: 627.3146s
	iters: 1100, epoch: 9 | loss: 0.3461719
	speed: 0.4361s/iter; left time: 518.5390s
Epoch: 9 cost time: 546.0163207054138
Epoch: 9, Steps: 1144 | Train Loss: 0.3050691 Vali Loss: 0.1663628 Test Loss: 0.2084510
Validation loss decreased (0.167057 --> 0.166363).  Saving model ...
Updating learning rate to 3.90625e-06
	iters: 100, epoch: 10 | loss: 0.3926701
	speed: 1.2188s/iter; left time: 1273.6563s
	iters: 200, epoch: 10 | loss: 0.4006556
	speed: 0.4685s/iter; left time: 442.7771s
	iters: 300, epoch: 10 | loss: 0.2389248
	speed: 0.4863s/iter; left time: 410.9321s
	iters: 400, epoch: 10 | loss: 0.2923461
	speed: 0.4863s/iter; left time: 362.3098s
	iters: 500, epoch: 10 | loss: 0.3319205
	speed: 0.4845s/iter; left time: 312.4816s
	iters: 600, epoch: 10 | loss: 0.2819532
	speed: 0.4859s/iter; left time: 264.8385s
	iters: 700, epoch: 10 | loss: 0.2925628
	speed: 0.4863s/iter; left time: 216.4207s
	iters: 800, epoch: 10 | loss: 0.3708101
	speed: 0.4864s/iter; left time: 167.8032s
	iters: 900, epoch: 10 | loss: 0.3056296
	speed: 0.4868s/iter; left time: 119.2681s
	iters: 1000, epoch: 10 | loss: 0.2798971
	speed: 0.4850s/iter; left time: 70.3230s
	iters: 1100, epoch: 10 | loss: 0.3679596
	speed: 0.4856s/iter; left time: 21.8528s
Epoch: 10 cost time: 547.689564704895
Epoch: 10, Steps: 1144 | Train Loss: 0.3047760 Vali Loss: 0.1667922 Test Loss: 0.2082165
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-06
>>>>>>>testing : AGPT_loss_solar_96_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10417
test shape: (10417, 96, 137) (10417, 96, 137)
test shape: (10417, 96, 137) (10417, 96, 137)
mse:0.2084805965423584, mae:0.2460179626941681
Running Solar with seq_len=192, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_192_96        Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_192_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36505
val 5161
test 10417
	iters: 100, epoch: 1 | loss: 0.5681509
	speed: 0.9439s/iter; left time: 10675.9506s
	iters: 200, epoch: 1 | loss: 0.3570771
	speed: 0.9434s/iter; left time: 10576.7105s
	iters: 300, epoch: 1 | loss: 0.3372493
	speed: 0.9446s/iter; left time: 10494.9125s
	iters: 400, epoch: 1 | loss: 0.3984600
	speed: 0.9436s/iter; left time: 10390.2463s
	iters: 500, epoch: 1 | loss: 0.2436541
	speed: 0.7941s/iter; left time: 8664.7266s
	iters: 600, epoch: 1 | loss: 0.2540791
	speed: 0.7997s/iter; left time: 8645.3419s
	iters: 700, epoch: 1 | loss: 0.3145329
	speed: 0.7877s/iter; left time: 8437.0394s
	iters: 800, epoch: 1 | loss: 0.4025060
	speed: 0.7789s/iter; left time: 8265.1426s
	iters: 900, epoch: 1 | loss: 0.2644581
	speed: 0.8037s/iter; left time: 8447.3752s
	iters: 1000, epoch: 1 | loss: 0.3257450
	speed: 0.9496s/iter; left time: 9886.7695s
	iters: 1100, epoch: 1 | loss: 0.3578220
	speed: 0.9483s/iter; left time: 9777.7414s
Epoch: 1 cost time: 1002.6253833770752
Epoch: 1, Steps: 1141 | Train Loss: 0.3829976 Vali Loss: 1.0472889 Test Loss: 0.8238440
Validation loss decreased (inf --> 1.047289).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.2745326
	speed: 2.4646s/iter; left time: 25064.8269s
	iters: 200, epoch: 2 | loss: 0.3182091
	speed: 0.9502s/iter; left time: 9568.4648s
	iters: 300, epoch: 2 | loss: 0.2975768
	speed: 0.9507s/iter; left time: 9478.8642s
	iters: 400, epoch: 2 | loss: 0.3108034
	speed: 0.9487s/iter; left time: 9363.4022s
	iters: 500, epoch: 2 | loss: 0.2656569
	speed: 0.9511s/iter; left time: 9292.2595s
	iters: 600, epoch: 2 | loss: 0.2669443
	speed: 0.9513s/iter; left time: 9198.6632s
	iters: 700, epoch: 2 | loss: 0.3651917
	speed: 0.9480s/iter; left time: 9072.6906s
	iters: 800, epoch: 2 | loss: 0.2434748
	speed: 0.9495s/iter; left time: 8991.4779s
	iters: 900, epoch: 2 | loss: 0.2660138
	speed: 0.8465s/iter; left time: 7931.8879s
	iters: 1000, epoch: 2 | loss: 0.2671156
	speed: 0.7916s/iter; left time: 7338.2637s
	iters: 1100, epoch: 2 | loss: 0.2721684
	speed: 0.8704s/iter; left time: 7981.2010s
Epoch: 2 cost time: 1049.5225496292114
Epoch: 2, Steps: 1141 | Train Loss: 0.2995272 Vali Loss: 0.2630230 Test Loss: 0.2562305
Validation loss decreased (1.047289 --> 0.263023).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.3066945
	speed: 2.4674s/iter; left time: 22278.4604s
	iters: 200, epoch: 3 | loss: 0.2721230
	speed: 0.9525s/iter; left time: 8505.2923s
	iters: 300, epoch: 3 | loss: 0.3453927
	speed: 0.9532s/iter; left time: 8415.7263s
	iters: 400, epoch: 3 | loss: 0.2256759
	speed: 0.9543s/iter; left time: 8329.8596s
	iters: 500, epoch: 3 | loss: 0.3258971
	speed: 0.9544s/iter; left time: 8235.7218s
	iters: 600, epoch: 3 | loss: 0.3063512
	speed: 0.9515s/iter; left time: 8115.1819s
	iters: 700, epoch: 3 | loss: 0.2712376
	speed: 0.9538s/iter; left time: 8039.3573s
	iters: 800, epoch: 3 | loss: 0.2612045
	speed: 0.9490s/iter; left time: 7904.5669s
	iters: 900, epoch: 3 | loss: 0.2637398
	speed: 0.9529s/iter; left time: 7841.6755s
	iters: 1000, epoch: 3 | loss: 0.2368052
	speed: 0.9530s/iter; left time: 7746.9259s
	iters: 1100, epoch: 3 | loss: 0.2723967
	speed: 0.8613s/iter; left time: 6915.2786s
Epoch: 3 cost time: 1070.0874786376953
Epoch: 3, Steps: 1141 | Train Loss: 0.2851758 Vali Loss: 0.1695404 Test Loss: 0.1921392
Validation loss decreased (0.263023 --> 0.169540).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.3059384
	speed: 2.2909s/iter; left time: 18070.9814s
	iters: 200, epoch: 4 | loss: 0.2830338
	speed: 0.9507s/iter; left time: 7404.2578s
	iters: 300, epoch: 4 | loss: 0.2748960
	speed: 0.9537s/iter; left time: 7331.8959s
	iters: 400, epoch: 4 | loss: 0.3319117
	speed: 0.9529s/iter; left time: 7230.6915s
	iters: 500, epoch: 4 | loss: 0.2383411
	speed: 0.9534s/iter; left time: 7139.1675s
	iters: 600, epoch: 4 | loss: 0.2833329
	speed: 0.9577s/iter; left time: 7075.6573s
	iters: 700, epoch: 4 | loss: 0.2479704
	speed: 0.9556s/iter; left time: 6964.6723s
	iters: 800, epoch: 4 | loss: 0.2558860
	speed: 0.9554s/iter; left time: 6867.3920s
	iters: 900, epoch: 4 | loss: 0.2808392
	speed: 0.9560s/iter; left time: 6775.8897s
	iters: 1000, epoch: 4 | loss: 0.2818339
	speed: 0.9538s/iter; left time: 6665.2530s
	iters: 1100, epoch: 4 | loss: 0.2678760
	speed: 0.9573s/iter; left time: 6593.6048s
Epoch: 4 cost time: 1088.2236549854279
Epoch: 4, Steps: 1141 | Train Loss: 0.2767967 Vali Loss: 0.1622525 Test Loss: 0.1892850
Validation loss decreased (0.169540 --> 0.162252).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 0.2482742
	speed: 2.2634s/iter; left time: 15271.2992s
	iters: 200, epoch: 5 | loss: 0.3253722
	speed: 0.7826s/iter; left time: 5201.6178s
	iters: 300, epoch: 5 | loss: 0.2540947
	speed: 0.9526s/iter; left time: 6236.4041s
	iters: 400, epoch: 5 | loss: 0.2686909
	speed: 0.9574s/iter; left time: 6172.3432s
	iters: 500, epoch: 5 | loss: 0.2841371
	speed: 0.9563s/iter; left time: 6069.4067s
	iters: 600, epoch: 5 | loss: 0.2911620
	speed: 0.9572s/iter; left time: 5979.8958s
	iters: 700, epoch: 5 | loss: 0.2415354
	speed: 0.9544s/iter; left time: 5866.6683s
	iters: 800, epoch: 5 | loss: 0.2308091
	speed: 0.9524s/iter; left time: 5759.2031s
	iters: 900, epoch: 5 | loss: 0.2861522
	speed: 0.9541s/iter; left time: 5673.8431s
	iters: 1000, epoch: 5 | loss: 0.2952424
	speed: 0.9543s/iter; left time: 5579.7822s
	iters: 1100, epoch: 5 | loss: 0.2660165
	speed: 0.9552s/iter; left time: 5489.5459s
Epoch: 5 cost time: 1054.6411328315735
Epoch: 5, Steps: 1141 | Train Loss: 0.2714420 Vali Loss: 0.1724075 Test Loss: 0.1999666
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-05
	iters: 100, epoch: 6 | loss: 0.2605385
	speed: 2.4931s/iter; left time: 13976.2414s
	iters: 200, epoch: 6 | loss: 0.2751208
	speed: 0.9197s/iter; left time: 5063.7413s
	iters: 300, epoch: 6 | loss: 0.2969300
	speed: 0.7806s/iter; left time: 4219.8924s
	iters: 400, epoch: 6 | loss: 0.2646661
	speed: 0.7849s/iter; left time: 4164.6776s
	iters: 500, epoch: 6 | loss: 0.2771466
	speed: 0.9194s/iter; left time: 4786.5982s
	iters: 600, epoch: 6 | loss: 0.2428353
	speed: 0.9524s/iter; left time: 4862.7625s
	iters: 700, epoch: 6 | loss: 0.3188774
	speed: 0.9532s/iter; left time: 4771.5397s
	iters: 800, epoch: 6 | loss: 0.2503891
	speed: 0.9527s/iter; left time: 4673.9719s
	iters: 900, epoch: 6 | loss: 0.2473788
	speed: 0.9542s/iter; left time: 4585.9275s
	iters: 1000, epoch: 6 | loss: 0.2282034
	speed: 0.9519s/iter; left time: 4479.6432s
	iters: 1100, epoch: 6 | loss: 0.3069690
	speed: 0.9507s/iter; left time: 4378.9200s
Epoch: 6 cost time: 1046.0872695446014
Epoch: 6, Steps: 1141 | Train Loss: 0.2675005 Vali Loss: 0.1501564 Test Loss: 0.1914989
Validation loss decreased (0.162252 --> 0.150156).  Saving model ...
Updating learning rate to 3.125e-05
	iters: 100, epoch: 7 | loss: 0.2583434
	speed: 2.4691s/iter; left time: 11024.6951s
	iters: 200, epoch: 7 | loss: 0.2301116
	speed: 0.9547s/iter; left time: 4167.4780s
	iters: 300, epoch: 7 | loss: 0.2631197
	speed: 0.9539s/iter; left time: 4068.3751s
	iters: 400, epoch: 7 | loss: 0.2928434
	speed: 0.9547s/iter; left time: 3976.5185s
	iters: 500, epoch: 7 | loss: 0.2149276
	speed: 0.7925s/iter; left time: 3221.4688s
	iters: 600, epoch: 7 | loss: 0.1962727
	speed: 0.7857s/iter; left time: 3115.1079s
	iters: 700, epoch: 7 | loss: 0.2182981
	speed: 0.8734s/iter; left time: 3375.5869s
	iters: 800, epoch: 7 | loss: 0.3298131
	speed: 0.9540s/iter; left time: 3591.7727s
	iters: 900, epoch: 7 | loss: 0.2554441
	speed: 0.9557s/iter; left time: 3502.7968s
	iters: 1000, epoch: 7 | loss: 0.3493542
	speed: 0.9517s/iter; left time: 3392.7397s
	iters: 1100, epoch: 7 | loss: 0.1776855
	speed: 0.9542s/iter; left time: 3306.3275s
Epoch: 7 cost time: 1046.8717653751373
Epoch: 7, Steps: 1141 | Train Loss: 0.2649622 Vali Loss: 0.1639712 Test Loss: 0.1910964
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 8 | loss: 0.2084054
	speed: 2.4713s/iter; left time: 8214.5231s
	iters: 200, epoch: 8 | loss: 0.2793321
	speed: 0.9535s/iter; left time: 3074.0212s
	iters: 300, epoch: 8 | loss: 0.2561732
	speed: 0.9529s/iter; left time: 2976.9430s
	iters: 400, epoch: 8 | loss: 0.2246278
	speed: 0.9533s/iter; left time: 2882.7810s
	iters: 500, epoch: 8 | loss: 0.2690647
	speed: 0.9555s/iter; left time: 2793.8816s
	iters: 600, epoch: 8 | loss: 0.2233825
	speed: 0.9553s/iter; left time: 2697.9055s
	iters: 700, epoch: 8 | loss: 0.2339644
	speed: 0.8389s/iter; left time: 2285.1545s
	iters: 800, epoch: 8 | loss: 0.2397356
	speed: 0.7793s/iter; left time: 2044.8914s
	iters: 900, epoch: 8 | loss: 0.2430349
	speed: 0.8330s/iter; left time: 2102.4833s
	iters: 1000, epoch: 8 | loss: 0.2809557
	speed: 0.9529s/iter; left time: 2309.9335s
	iters: 1100, epoch: 8 | loss: 0.2706984
	speed: 0.9562s/iter; left time: 2222.1092s
Epoch: 8 cost time: 1047.0702040195465
Epoch: 8, Steps: 1141 | Train Loss: 0.2633610 Vali Loss: 0.1630188 Test Loss: 0.1941462
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 9 | loss: 0.2611513
	speed: 2.4814s/iter; left time: 5416.9160s
	iters: 200, epoch: 9 | loss: 0.3102933
	speed: 0.9513s/iter; left time: 1981.6195s
	iters: 300, epoch: 9 | loss: 0.2432538
	speed: 0.9533s/iter; left time: 1890.4878s
	iters: 400, epoch: 9 | loss: 0.2887319
	speed: 0.9517s/iter; left time: 1791.9922s
	iters: 500, epoch: 9 | loss: 0.2779877
	speed: 0.9527s/iter; left time: 1698.7143s
	iters: 600, epoch: 9 | loss: 0.3655723
	speed: 0.9520s/iter; left time: 1602.2692s
	iters: 700, epoch: 9 | loss: 0.2583869
	speed: 0.9537s/iter; left time: 1509.7637s
	iters: 800, epoch: 9 | loss: 0.2531646
	speed: 0.9528s/iter; left time: 1413.0111s
	iters: 900, epoch: 9 | loss: 0.3175512
	speed: 0.8803s/iter; left time: 1217.4448s
	iters: 1000, epoch: 9 | loss: 0.2806899
	speed: 0.7804s/iter; left time: 1001.1985s
	iters: 1100, epoch: 9 | loss: 0.2411375
	speed: 0.7924s/iter; left time: 937.4115s
Epoch: 9 cost time: 1045.91237282753
Epoch: 9, Steps: 1141 | Train Loss: 0.2623785 Vali Loss: 0.1630338 Test Loss: 0.1920566
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_solar_192_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10417
test shape: (10417, 96, 137) (10417, 96, 137)
test shape: (10417, 96, 137) (10417, 96, 137)
mse:0.19090186059474945, mae:0.24454046785831451
Running Solar with seq_len=192, pred_len=192
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_192_192       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_192_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36409
val 5065
test 10321
	iters: 100, epoch: 1 | loss: 0.4256728
	speed: 0.9573s/iter; left time: 10799.1985s
	iters: 200, epoch: 1 | loss: 0.4057180
	speed: 0.9525s/iter; left time: 10650.2092s
	iters: 300, epoch: 1 | loss: 0.3398718
	speed: 0.9547s/iter; left time: 10578.7688s
	iters: 400, epoch: 1 | loss: 0.3374957
	speed: 0.9549s/iter; left time: 10486.0344s
	iters: 500, epoch: 1 | loss: 0.3285949
	speed: 0.9547s/iter; left time: 10387.7353s
	iters: 600, epoch: 1 | loss: 0.2869412
	speed: 0.9560s/iter; left time: 10306.7412s
	iters: 700, epoch: 1 | loss: 0.3130696
	speed: 0.9539s/iter; left time: 10188.9787s
	iters: 800, epoch: 1 | loss: 0.3252028
	speed: 0.9555s/iter; left time: 10109.7026s
	iters: 900, epoch: 1 | loss: 0.2732561
	speed: 0.9484s/iter; left time: 9940.2798s
	iters: 1000, epoch: 1 | loss: 0.3042220
	speed: 0.7838s/iter; left time: 8136.5107s
	iters: 1100, epoch: 1 | loss: 0.3006274
	speed: 0.7871s/iter; left time: 8092.1142s
Epoch: 1 cost time: 1046.510026216507
Epoch: 1, Steps: 1138 | Train Loss: 0.3560230 Vali Loss: 0.2131027 Test Loss: 0.2403872
Validation loss decreased (inf --> 0.213103).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.3023203
	speed: 2.3716s/iter; left time: 24055.2145s
	iters: 200, epoch: 2 | loss: 0.3452375
	speed: 0.9566s/iter; left time: 9607.4983s
	iters: 300, epoch: 2 | loss: 0.3086877
	speed: 0.9570s/iter; left time: 9515.0563s
	iters: 400, epoch: 2 | loss: 0.2754931
	speed: 0.9536s/iter; left time: 9386.2422s
	iters: 500, epoch: 2 | loss: 0.3104624
	speed: 0.9547s/iter; left time: 9301.7662s
	iters: 600, epoch: 2 | loss: 0.2985381
	speed: 0.9552s/iter; left time: 9211.3554s
	iters: 700, epoch: 2 | loss: 0.3151658
	speed: 0.9535s/iter; left time: 9099.0509s
	iters: 800, epoch: 2 | loss: 0.2672218
	speed: 0.9536s/iter; left time: 9004.8545s
	iters: 900, epoch: 2 | loss: 0.2748986
	speed: 0.9527s/iter; left time: 8900.6575s
	iters: 1000, epoch: 2 | loss: 0.2425353
	speed: 0.9547s/iter; left time: 8824.4017s
	iters: 1100, epoch: 2 | loss: 0.3376532
	speed: 0.9560s/iter; left time: 8740.8227s
Epoch: 2 cost time: 1084.9558868408203
Epoch: 2, Steps: 1138 | Train Loss: 0.2923765 Vali Loss: 0.1885072 Test Loss: 0.2179675
Validation loss decreased (0.213103 --> 0.188507).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.2550569
	speed: 2.1535s/iter; left time: 19392.5059s
	iters: 200, epoch: 3 | loss: 0.3371950
	speed: 0.9515s/iter; left time: 8473.0867s
	iters: 300, epoch: 3 | loss: 0.2760546
	speed: 0.9534s/iter; left time: 8394.5512s
	iters: 400, epoch: 3 | loss: 0.3171898
	speed: 0.9546s/iter; left time: 8309.7545s
	iters: 500, epoch: 3 | loss: 0.2818240
	speed: 0.9552s/iter; left time: 8219.0840s
	iters: 600, epoch: 3 | loss: 0.2822563
	speed: 0.9571s/iter; left time: 8140.2839s
	iters: 700, epoch: 3 | loss: 0.2903892
	speed: 0.9557s/iter; left time: 8032.7193s
	iters: 800, epoch: 3 | loss: 0.2814839
	speed: 0.9554s/iter; left time: 7934.2658s
	iters: 900, epoch: 3 | loss: 0.3148555
	speed: 0.9545s/iter; left time: 7831.9038s
	iters: 1000, epoch: 3 | loss: 0.2719659
	speed: 0.9539s/iter; left time: 7731.2151s
	iters: 1100, epoch: 3 | loss: 0.3417382
	speed: 0.9535s/iter; left time: 7632.9476s
Epoch: 3 cost time: 1073.2280187606812
Epoch: 3, Steps: 1138 | Train Loss: 0.2787629 Vali Loss: 0.1966302 Test Loss: 0.2304338
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.2621380
	speed: 2.3897s/iter; left time: 18800.0008s
	iters: 200, epoch: 4 | loss: 0.2850564
	speed: 0.7836s/iter; left time: 6085.9511s
	iters: 300, epoch: 4 | loss: 0.2449547
	speed: 0.7854s/iter; left time: 6021.5740s
	iters: 400, epoch: 4 | loss: 0.2682967
	speed: 0.7746s/iter; left time: 5861.5287s
	iters: 500, epoch: 4 | loss: 0.2752526
	speed: 0.6853s/iter; left time: 5116.8708s
	iters: 600, epoch: 4 | loss: 0.2795119
	speed: 0.8162s/iter; left time: 6012.8945s
	iters: 700, epoch: 4 | loss: 0.2777538
	speed: 0.9578s/iter; left time: 6960.2500s
	iters: 800, epoch: 4 | loss: 0.2877994
	speed: 0.9570s/iter; left time: 6859.0350s
	iters: 900, epoch: 4 | loss: 0.2868899
	speed: 0.9560s/iter; left time: 6756.2860s
	iters: 1000, epoch: 4 | loss: 0.2799301
	speed: 0.9583s/iter; left time: 6676.2377s
	iters: 1100, epoch: 4 | loss: 0.3020826
	speed: 0.9582s/iter; left time: 6579.8951s
Epoch: 4 cost time: 990.0745058059692
Epoch: 4, Steps: 1138 | Train Loss: 0.2721865 Vali Loss: 0.1724442 Test Loss: 0.2080261
Validation loss decreased (0.188507 --> 0.172444).  Saving model ...
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 0.2501442
	speed: 2.4557s/iter; left time: 16524.4497s
	iters: 200, epoch: 5 | loss: 0.2538535
	speed: 0.9566s/iter; left time: 6341.3916s
	iters: 300, epoch: 5 | loss: 0.2343021
	speed: 0.9573s/iter; left time: 6250.1408s
	iters: 400, epoch: 5 | loss: 0.2715798
	speed: 0.9560s/iter; left time: 6146.2703s
	iters: 500, epoch: 5 | loss: 0.2596809
	speed: 0.9563s/iter; left time: 6052.2390s
	iters: 600, epoch: 5 | loss: 0.2805556
	speed: 0.8399s/iter; left time: 5231.8008s
	iters: 700, epoch: 5 | loss: 0.2512195
	speed: 0.7996s/iter; left time: 4900.5389s
	iters: 800, epoch: 5 | loss: 0.2343723
	speed: 0.8893s/iter; left time: 5361.8787s
	iters: 900, epoch: 5 | loss: 0.2807633
	speed: 0.9564s/iter; left time: 5670.7845s
	iters: 1000, epoch: 5 | loss: 0.2694461
	speed: 0.9577s/iter; left time: 5582.4830s
	iters: 1100, epoch: 5 | loss: 0.2739277
	speed: 0.9595s/iter; left time: 5496.8793s
Epoch: 5 cost time: 1054.716614484787
Epoch: 5, Steps: 1138 | Train Loss: 0.2672529 Vali Loss: 0.1724783 Test Loss: 0.2055825
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-05
	iters: 100, epoch: 6 | loss: 0.2457607
	speed: 2.4455s/iter; left time: 13672.6083s
	iters: 200, epoch: 6 | loss: 0.2825175
	speed: 0.9613s/iter; left time: 5278.6585s
	iters: 300, epoch: 6 | loss: 0.2991276
	speed: 0.9560s/iter; left time: 5153.7791s
	iters: 400, epoch: 6 | loss: 0.2257413
	speed: 0.9573s/iter; left time: 5064.9460s
	iters: 500, epoch: 6 | loss: 0.2610649
	speed: 0.9573s/iter; left time: 4969.5096s
	iters: 600, epoch: 6 | loss: 0.2352275
	speed: 0.9577s/iter; left time: 4875.5629s
	iters: 700, epoch: 6 | loss: 0.2370597
	speed: 0.9582s/iter; left time: 4782.4754s
	iters: 800, epoch: 6 | loss: 0.2445031
	speed: 0.8547s/iter; left time: 4180.1231s
	iters: 900, epoch: 6 | loss: 0.2914353
	speed: 0.7978s/iter; left time: 3822.4782s
	iters: 1000, epoch: 6 | loss: 0.2837136
	speed: 0.8609s/iter; left time: 4038.7080s
	iters: 1100, epoch: 6 | loss: 0.2608691
	speed: 0.9560s/iter; left time: 4388.8507s
Epoch: 6 cost time: 1053.605750799179
Epoch: 6, Steps: 1138 | Train Loss: 0.2641097 Vali Loss: 0.1674713 Test Loss: 0.2036760
Validation loss decreased (0.172444 --> 0.167471).  Saving model ...
Updating learning rate to 3.125e-05
	iters: 100, epoch: 7 | loss: 0.2287345
	speed: 2.4505s/iter; left time: 10912.1351s
	iters: 200, epoch: 7 | loss: 0.2242621
	speed: 0.9577s/iter; left time: 4168.9446s
	iters: 300, epoch: 7 | loss: 0.2517594
	speed: 0.9580s/iter; left time: 4074.3402s
	iters: 400, epoch: 7 | loss: 0.2647998
	speed: 0.9569s/iter; left time: 3973.9904s
	iters: 500, epoch: 7 | loss: 0.2686130
	speed: 0.9601s/iter; left time: 3891.1885s
	iters: 600, epoch: 7 | loss: 0.2658669
	speed: 0.9599s/iter; left time: 3794.3229s
	iters: 700, epoch: 7 | loss: 0.3160912
	speed: 0.9585s/iter; left time: 3693.0984s
	iters: 800, epoch: 7 | loss: 0.2754325
	speed: 0.9589s/iter; left time: 3598.7360s
	iters: 900, epoch: 7 | loss: 0.2647449
	speed: 0.9594s/iter; left time: 3504.6894s
	iters: 1000, epoch: 7 | loss: 0.2680465
	speed: 0.8851s/iter; left time: 3144.8645s
	iters: 1100, epoch: 7 | loss: 0.2377115
	speed: 0.7932s/iter; left time: 2738.7858s
Epoch: 7 cost time: 1061.41561293602
Epoch: 7, Steps: 1138 | Train Loss: 0.2620108 Vali Loss: 0.1678540 Test Loss: 0.2079526
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-05
	iters: 100, epoch: 8 | loss: 0.2942857
	speed: 2.3663s/iter; left time: 7844.3711s
	iters: 200, epoch: 8 | loss: 0.2955043
	speed: 0.9580s/iter; left time: 3080.1202s
	iters: 300, epoch: 8 | loss: 0.2496150
	speed: 0.9590s/iter; left time: 2987.4029s
	iters: 400, epoch: 8 | loss: 0.2966366
	speed: 0.9594s/iter; left time: 2892.4861s
	iters: 500, epoch: 8 | loss: 0.2418014
	speed: 0.9586s/iter; left time: 2794.4028s
	iters: 600, epoch: 8 | loss: 0.2617228
	speed: 0.9574s/iter; left time: 2695.0309s
	iters: 700, epoch: 8 | loss: 0.2684908
	speed: 0.9574s/iter; left time: 2599.3333s
	iters: 800, epoch: 8 | loss: 0.2594322
	speed: 0.9605s/iter; left time: 2511.7730s
	iters: 900, epoch: 8 | loss: 0.2456093
	speed: 0.9608s/iter; left time: 2416.5010s
	iters: 1000, epoch: 8 | loss: 0.2479638
	speed: 0.9593s/iter; left time: 2316.7791s
	iters: 1100, epoch: 8 | loss: 0.2408821
	speed: 0.9585s/iter; left time: 2218.8508s
Epoch: 8 cost time: 1091.0896935462952
Epoch: 8, Steps: 1138 | Train Loss: 0.2606827 Vali Loss: 0.1710065 Test Loss: 0.2101929
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-06
	iters: 100, epoch: 9 | loss: 0.2783633
	speed: 2.2013s/iter; left time: 4792.1334s
	iters: 200, epoch: 9 | loss: 0.2703164
	speed: 0.9585s/iter; left time: 1990.7553s
	iters: 300, epoch: 9 | loss: 0.2538219
	speed: 0.9564s/iter; left time: 1890.7990s
	iters: 400, epoch: 9 | loss: 0.2488083
	speed: 0.9574s/iter; left time: 1796.9976s
	iters: 500, epoch: 9 | loss: 0.2304497
	speed: 0.9573s/iter; left time: 1701.0560s
	iters: 600, epoch: 9 | loss: 0.2761846
	speed: 0.9582s/iter; left time: 1606.8255s
	iters: 700, epoch: 9 | loss: 0.2630240
	speed: 0.9585s/iter; left time: 1511.4776s
	iters: 800, epoch: 9 | loss: 0.2366395
	speed: 0.9568s/iter; left time: 1413.2477s
	iters: 900, epoch: 9 | loss: 0.2571985
	speed: 0.9560s/iter; left time: 1316.4068s
	iters: 1000, epoch: 9 | loss: 0.2787945
	speed: 0.9573s/iter; left time: 1222.4331s
	iters: 1100, epoch: 9 | loss: 0.2681643
	speed: 0.9563s/iter; left time: 1125.5760s
Epoch: 9 cost time: 1075.4490168094635
Epoch: 9, Steps: 1138 | Train Loss: 0.2599431 Vali Loss: 0.1716166 Test Loss: 0.2085938
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_solar_192_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10321
test shape: (10321, 192, 137) (10321, 192, 137)
test shape: (10321, 192, 137) (10321, 192, 137)
mse:0.2036290317773819, mae:0.25157448649406433
Running Solar with seq_len=288, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_288_96        Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_288_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36409
val 5161
test 10417
	iters: 100, epoch: 1 | loss: 0.5680515
	speed: 1.0018s/iter; left time: 11301.1844s
	iters: 200, epoch: 1 | loss: 0.3377350
	speed: 1.0726s/iter; left time: 11992.9406s
	iters: 300, epoch: 1 | loss: 0.3159784
	speed: 1.0719s/iter; left time: 11877.7552s
	iters: 400, epoch: 1 | loss: 0.3518483
	speed: 1.0776s/iter; left time: 11833.4317s
	iters: 500, epoch: 1 | loss: 0.3409297
	speed: 1.0819s/iter; left time: 11772.1273s
	iters: 600, epoch: 1 | loss: 0.2581717
	speed: 1.0764s/iter; left time: 11605.1237s
	iters: 700, epoch: 1 | loss: 0.3689403
	speed: 1.0766s/iter; left time: 11499.2450s
	iters: 800, epoch: 1 | loss: 0.2785257
	speed: 1.0725s/iter; left time: 11347.7971s
	iters: 900, epoch: 1 | loss: 0.2659878
	speed: 1.0725s/iter; left time: 11241.1435s
	iters: 1000, epoch: 1 | loss: 0.2495212
	speed: 1.0732s/iter; left time: 11141.0843s
	iters: 1100, epoch: 1 | loss: 0.1949839
	speed: 1.0732s/iter; left time: 11033.5788s
Epoch: 1 cost time: 1215.7272210121155
Epoch: 1, Steps: 1138 | Train Loss: 0.3792559 Vali Loss: 0.1754992 Test Loss: 0.2122810
Validation loss decreased (inf --> 0.175499).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.2598900
	speed: 2.8801s/iter; left time: 29212.8622s
	iters: 200, epoch: 2 | loss: 0.2809150
	speed: 1.0740s/iter; left time: 10786.2798s
	iters: 300, epoch: 2 | loss: 0.2507341
	speed: 1.0787s/iter; left time: 10725.9939s
	iters: 400, epoch: 2 | loss: 0.2458968
	speed: 1.0750s/iter; left time: 10580.7497s
	iters: 500, epoch: 2 | loss: 0.2419091
	speed: 1.0799s/iter; left time: 10521.2757s
	iters: 600, epoch: 2 | loss: 0.2287606
	speed: 1.0797s/iter; left time: 10411.3192s
	iters: 700, epoch: 2 | loss: 0.2329532
	speed: 1.0748s/iter; left time: 10257.2691s
	iters: 800, epoch: 2 | loss: 0.2193060
	speed: 1.0695s/iter; left time: 10098.9374s
	iters: 900, epoch: 2 | loss: 0.2432732
	speed: 1.0714s/iter; left time: 10009.6657s
	iters: 1000, epoch: 2 | loss: 0.2762358
	speed: 1.0760s/iter; left time: 9945.4387s
	iters: 1100, epoch: 2 | loss: 7.1086025
	speed: 1.0756s/iter; left time: 9834.3153s
Epoch: 2 cost time: 1216.7520644664764
Epoch: 2, Steps: 1138 | Train Loss: 0.8962956 Vali Loss: 346.3858337 Test Loss: 258.5855408
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 2.6820533
	speed: 2.9012s/iter; left time: 26125.0917s
	iters: 200, epoch: 3 | loss: 1.7692217
	speed: 1.0759s/iter; left time: 9580.5469s
	iters: 300, epoch: 3 | loss: 1.3176515
	speed: 1.0694s/iter; left time: 9416.2923s
	iters: 400, epoch: 3 | loss: 1.0790794
	speed: 1.0740s/iter; left time: 9349.3763s
	iters: 500, epoch: 3 | loss: 1.0354170
	speed: 1.0738s/iter; left time: 9239.7747s
	iters: 600, epoch: 3 | loss: 0.8895216
	speed: 1.0697s/iter; left time: 9097.4281s
	iters: 700, epoch: 3 | loss: 0.9511508
	speed: 1.0738s/iter; left time: 9025.6866s
	iters: 800, epoch: 3 | loss: 0.9674910
	speed: 1.0710s/iter; left time: 8894.7798s
	iters: 900, epoch: 3 | loss: 0.9132177
	speed: 1.0716s/iter; left time: 8792.3431s
	iters: 1000, epoch: 3 | loss: 0.7801161
	speed: 1.0758s/iter; left time: 8718.9730s
	iters: 1100, epoch: 3 | loss: 0.7327564
	speed: 1.0675s/iter; left time: 8545.3388s
Epoch: 3 cost time: 1216.132642507553
Epoch: 3, Steps: 1138 | Train Loss: 1.3253360 Vali Loss: 86.1284637 Test Loss: 64.2691574
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.5276509
	speed: 2.8782s/iter; left time: 22642.9945s
	iters: 200, epoch: 4 | loss: 0.5254815
	speed: 1.0795s/iter; left time: 8384.6945s
	iters: 300, epoch: 4 | loss: 0.4832318
	speed: 1.0773s/iter; left time: 8259.3696s
	iters: 400, epoch: 4 | loss: 0.4473806
	speed: 1.0755s/iter; left time: 8138.2889s
	iters: 500, epoch: 4 | loss: 0.4891858
	speed: 1.0742s/iter; left time: 8020.8843s
	iters: 600, epoch: 4 | loss: 0.5284243
	speed: 1.0718s/iter; left time: 7896.2571s
	iters: 700, epoch: 4 | loss: 0.5138782
	speed: 1.0732s/iter; left time: 7799.1452s
	iters: 800, epoch: 4 | loss: 0.4424685
	speed: 1.0797s/iter; left time: 7737.9271s
	iters: 900, epoch: 4 | loss: 0.4690390
	speed: 1.0774s/iter; left time: 7613.9220s
	iters: 1000, epoch: 4 | loss: 0.4758023
	speed: 1.0723s/iter; left time: 7470.8230s
	iters: 1100, epoch: 4 | loss: 0.4091752
	speed: 1.0751s/iter; left time: 7383.0285s
Epoch: 4 cost time: 1216.55761551857
Epoch: 4, Steps: 1138 | Train Loss: 0.4970060 Vali Loss: 73.7399902 Test Loss: 54.9922791
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_solar_288_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10417
test shape: (10417, 96, 137) (10417, 96, 137)
test shape: (10417, 96, 137) (10417, 96, 137)
mse:0.2121119350194931, mae:0.2754914164543152
Running Solar with seq_len=288, pred_len=192
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_288_192       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_288_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36313
val 5065
test 10321
	iters: 100, epoch: 1 | loss: 0.5834275
	speed: 1.0845s/iter; left time: 12201.2418s
	iters: 200, epoch: 1 | loss: 0.4591542
	speed: 1.0741s/iter; left time: 11976.9584s
	iters: 300, epoch: 1 | loss: 0.3620115
	speed: 1.0747s/iter; left time: 11875.9702s
	iters: 400, epoch: 1 | loss: 0.2781281
	speed: 1.0749s/iter; left time: 11771.1025s
	iters: 500, epoch: 1 | loss: 0.3795385
	speed: 1.0816s/iter; left time: 11736.7679s
	iters: 600, epoch: 1 | loss: 0.3022606
	speed: 1.0721s/iter; left time: 11525.9475s
	iters: 700, epoch: 1 | loss: 0.2786012
	speed: 1.0762s/iter; left time: 11462.6123s
	iters: 800, epoch: 1 | loss: 0.2428811
	speed: 1.0777s/iter; left time: 11371.0147s
	iters: 900, epoch: 1 | loss: 0.3287122
	speed: 1.0593s/iter; left time: 11070.9938s
	iters: 1000, epoch: 1 | loss: 0.2770696
	speed: 0.8662s/iter; left time: 8966.0957s
	iters: 1100, epoch: 1 | loss: 0.3328854
	speed: 0.8920s/iter; left time: 9144.0448s
Epoch: 1 cost time: 1173.5416419506073
Epoch: 1, Steps: 1135 | Train Loss: 0.4104467 Vali Loss: 0.1798088 Test Loss: 0.2346244
Validation loss decreased (inf --> 0.179809).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.2851469
	speed: 2.3903s/iter; left time: 24180.0620s
	iters: 200, epoch: 2 | loss: 0.2872158
	speed: 1.0711s/iter; left time: 10728.4669s
	iters: 300, epoch: 2 | loss: 0.2895570
	speed: 1.0719s/iter; left time: 10629.0965s
	iters: 400, epoch: 2 | loss: 0.2884078
	speed: 1.0713s/iter; left time: 10515.4970s
	iters: 500, epoch: 2 | loss: 0.2937895
	speed: 1.0723s/iter; left time: 10418.4753s
	iters: 600, epoch: 2 | loss: 0.3107103
	speed: 1.0737s/iter; left time: 10324.8138s
	iters: 700, epoch: 2 | loss: 0.2893961
	speed: 1.0755s/iter; left time: 10234.4098s
	iters: 800, epoch: 2 | loss: 0.2493022
	speed: 1.0715s/iter; left time: 10089.0970s
	iters: 900, epoch: 2 | loss: 0.2618190
	speed: 1.0711s/iter; left time: 9978.5202s
	iters: 1000, epoch: 2 | loss: 0.2840636
	speed: 1.0742s/iter; left time: 9899.6206s
	iters: 1100, epoch: 2 | loss: 0.2860423
	speed: 1.0706s/iter; left time: 9759.9721s
Epoch: 2 cost time: 1215.4692778587341
Epoch: 2, Steps: 1135 | Train Loss: 0.2819722 Vali Loss: 0.1742413 Test Loss: 0.2135919
Validation loss decreased (0.179809 --> 0.174241).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.2570316
	speed: 2.8770s/iter; left time: 25838.5510s
	iters: 200, epoch: 3 | loss: 0.2912344
	speed: 1.0652s/iter; left time: 9460.1083s
	iters: 300, epoch: 3 | loss: 0.2442998
	speed: 1.0667s/iter; left time: 9366.6658s
	iters: 400, epoch: 3 | loss: 0.2923613
	speed: 1.0694s/iter; left time: 9283.5765s
	iters: 500, epoch: 3 | loss: 0.3017372
	speed: 1.0686s/iter; left time: 9169.5805s
	iters: 600, epoch: 3 | loss: 0.2749801
	speed: 1.0734s/iter; left time: 9103.6265s
	iters: 700, epoch: 3 | loss: 0.2530901
	speed: 1.0696s/iter; left time: 8964.6678s
	iters: 800, epoch: 3 | loss: 0.3201375
	speed: 1.0696s/iter; left time: 8857.2992s
	iters: 900, epoch: 3 | loss: 0.2711627
	speed: 1.0677s/iter; left time: 8735.2028s
	iters: 1000, epoch: 3 | loss: 0.2725562
	speed: 1.0763s/iter; left time: 8697.4959s
	iters: 1100, epoch: 3 | loss: 0.2980475
	speed: 1.0744s/iter; left time: 8574.5511s
Epoch: 3 cost time: 1208.459017276764
Epoch: 3, Steps: 1135 | Train Loss: 0.2730165 Vali Loss: 1.6604621 Test Loss: 1.3217648
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.2531397
	speed: 2.8318s/iter; left time: 22218.6653s
	iters: 200, epoch: 4 | loss: 0.2540099
	speed: 1.0720s/iter; left time: 8303.7740s
	iters: 300, epoch: 4 | loss: 0.2672312
	speed: 1.0740s/iter; left time: 8211.5016s
	iters: 400, epoch: 4 | loss: 0.2691126
	speed: 1.0676s/iter; left time: 8056.4071s
	iters: 500, epoch: 4 | loss: 0.2650098
	speed: 1.0702s/iter; left time: 7968.5887s
	iters: 600, epoch: 4 | loss: 0.2340700
	speed: 1.0711s/iter; left time: 7867.9508s
	iters: 700, epoch: 4 | loss: 0.2709957
	speed: 1.0687s/iter; left time: 7743.5682s
	iters: 800, epoch: 4 | loss: 0.2555403
	speed: 1.0686s/iter; left time: 7636.5634s
	iters: 900, epoch: 4 | loss: 0.3064898
	speed: 1.0807s/iter; left time: 7614.6071s
	iters: 1000, epoch: 4 | loss: 0.2605782
	speed: 1.0768s/iter; left time: 7479.7169s
	iters: 1100, epoch: 4 | loss: 0.2738028
	speed: 1.0617s/iter; left time: 7268.4423s
Epoch: 4 cost time: 1207.7647125720978
Epoch: 4, Steps: 1135 | Train Loss: 0.2605389 Vali Loss: 1.0925277 Test Loss: 0.8925112
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.000125
	iters: 100, epoch: 5 | loss: 0.2726631
	speed: 2.8498s/iter; left time: 19124.9879s
	iters: 200, epoch: 5 | loss: 0.2307997
	speed: 1.0740s/iter; left time: 7100.1318s
	iters: 300, epoch: 5 | loss: 0.2286811
	speed: 1.0725s/iter; left time: 6983.2052s
	iters: 400, epoch: 5 | loss: 0.2620182
	speed: 1.0686s/iter; left time: 6851.0816s
	iters: 500, epoch: 5 | loss: 0.2305794
	speed: 1.0663s/iter; left time: 6729.2882s
	iters: 600, epoch: 5 | loss: 0.2269351
	speed: 1.0738s/iter; left time: 6669.4813s
	iters: 700, epoch: 5 | loss: 0.2261191
	speed: 1.0695s/iter; left time: 6535.4412s
	iters: 800, epoch: 5 | loss: 0.2320782
	speed: 1.0723s/iter; left time: 6445.6366s
	iters: 900, epoch: 5 | loss: 0.2233057
	speed: 1.0678s/iter; left time: 6311.9278s
	iters: 1000, epoch: 5 | loss: 0.2298962
	speed: 1.0704s/iter; left time: 6220.3581s
	iters: 1100, epoch: 5 | loss: 0.2410319
	speed: 1.0253s/iter; left time: 5855.5867s
Epoch: 5 cost time: 1203.7362682819366
Epoch: 5, Steps: 1135 | Train Loss: 0.2542475 Vali Loss: 0.9167396 Test Loss: 0.7667081
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : AGPT_loss_solar_288_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 10321
test shape: (10321, 192, 137) (10321, 192, 137)
test shape: (10321, 192, 137) (10321, 192, 137)
mse:0.2133542150259018, mae:0.2716928720474243
Running Solar with seq_len=384, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_384_96        Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_384_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36313
val 5161
test 10417
	iters: 100, epoch: 1 | loss: 0.7259599
	speed: 1.3491s/iter; left time: 15178.8078s
	iters: 200, epoch: 1 | loss: 0.3809049
	speed: 1.3364s/iter; left time: 14902.1297s
	iters: 300, epoch: 1 | loss: 0.3356753
	speed: 1.3363s/iter; left time: 14767.2007s
	iters: 400, epoch: 1 | loss: 0.3051928
	speed: 1.3306s/iter; left time: 14571.2176s
	iters: 500, epoch: 1 | loss: 0.3534896
	speed: 1.3377s/iter; left time: 14515.0477s
	iters: 600, epoch: 1 | loss: 0.2818576
	speed: 1.3362s/iter; left time: 14365.8457s
	iters: 700, epoch: 1 | loss: 0.3589708
	speed: 1.3167s/iter; left time: 14024.5771s
	iters: 800, epoch: 1 | loss: 0.2614500
	speed: 1.0843s/iter; left time: 11440.1056s
	iters: 900, epoch: 1 | loss: 0.3239159
	speed: 1.2166s/iter; left time: 12714.9482s
	iters: 1000, epoch: 1 | loss: 0.2748624
	speed: 1.3365s/iter; left time: 13833.6350s
	iters: 1100, epoch: 1 | loss: 0.3191392
	speed: 1.3354s/iter; left time: 13688.7037s
Epoch: 1 cost time: 1478.9095482826233
Epoch: 1, Steps: 1135 | Train Loss: 0.4457677 Vali Loss: 0.2670627 Test Loss: 0.2813784
Validation loss decreased (inf --> 0.267063).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.3277493
	speed: 3.9752s/iter; left time: 40212.6653s
	iters: 200, epoch: 2 | loss: 0.2891733
	speed: 1.3396s/iter; left time: 13417.7790s
	iters: 300, epoch: 2 | loss: 0.2646414
	speed: 1.3380s/iter; left time: 13267.2524s
	iters: 400, epoch: 2 | loss: 0.2595633
	speed: 1.3348s/iter; left time: 13102.1639s
	iters: 500, epoch: 2 | loss: 0.3029405
	speed: 1.1634s/iter; left time: 11303.4337s
	iters: 600, epoch: 2 | loss: 0.2511976
	speed: 1.1240s/iter; left time: 10808.0275s
	iters: 700, epoch: 2 | loss: 0.2855194
	speed: 1.3382s/iter; left time: 12734.0404s
	iters: 800, epoch: 2 | loss: 0.2613493
	speed: 1.3397s/iter; left time: 12614.9241s
	iters: 900, epoch: 2 | loss: 0.2918377
	speed: 1.3361s/iter; left time: 12447.2914s
	iters: 1000, epoch: 2 | loss: 0.2721151
	speed: 1.3402s/iter; left time: 12351.4197s
	iters: 1100, epoch: 2 | loss: 0.3132820
	speed: 1.3397s/iter; left time: 12212.5365s
Epoch: 2 cost time: 1479.906349658966
Epoch: 2, Steps: 1135 | Train Loss: 0.2769897 Vali Loss: 0.1872901 Test Loss: 0.2146230
Validation loss decreased (0.267063 --> 0.187290).  Saving model ...
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.2092523
	speed: 3.9562s/iter; left time: 35530.9105s
	iters: 200, epoch: 3 | loss: 0.2308466
	speed: 1.2806s/iter; left time: 11372.7102s
	iters: 300, epoch: 3 | loss: 0.2266197
	speed: 1.0843s/iter; left time: 9521.4812s
	iters: 400, epoch: 3 | loss: 0.2407118
	speed: 1.2684s/iter; left time: 11010.6443s
	iters: 500, epoch: 3 | loss: 0.2463921
	speed: 1.3324s/iter; left time: 11433.7347s
	iters: 600, epoch: 3 | loss: 0.3250553
	speed: 1.3386s/iter; left time: 11353.0879s
	iters: 700, epoch: 3 | loss: 0.2604848
	speed: 1.3391s/iter; left time: 11222.9929s
	iters: 800, epoch: 3 | loss: 0.2535124
	speed: 1.3398s/iter; left time: 11094.8251s
	iters: 900, epoch: 3 | loss: 0.2818059
	speed: 1.3440s/iter; left time: 10995.6652s
	iters: 1000, epoch: 3 | loss: 0.2297309
	speed: 1.3379s/iter; left time: 10811.8096s
	iters: 1100, epoch: 3 | loss: 0.2381181
	speed: 1.3381s/iter; left time: 10679.1864s
Epoch: 3 cost time: 1480.6206631660461
Epoch: 3, Steps: 1135 | Train Loss: 0.2619982 Vali Loss: 0.1600847 Test Loss: 0.1908011
Validation loss decreased (0.187290 --> 0.160085).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.2291785
	speed: 3.5957s/iter; left time: 28211.9610s
	iters: 200, epoch: 4 | loss: 0.2214030
	speed: 1.3407s/iter; left time: 10384.9269s
	iters: 300, epoch: 4 | loss: 0.2665794
	speed: 1.3409s/iter; left time: 10252.5424s
	iters: 400, epoch: 4 | loss: 0.2398095
	speed: 1.3383s/iter; left time: 10098.9938s
	iters: 500, epoch: 4 | loss: 0.2326073
	speed: 1.3419s/iter; left time: 9991.7556s
	iters: 600, epoch: 4 | loss: 0.2320431
	speed: 1.3402s/iter; left time: 9844.9349s
	iters: 700, epoch: 4 | loss: 0.2818424
	speed: 1.3359s/iter; left time: 9680.0444s
	iters: 800, epoch: 4 | loss: 0.2401073
	speed: 1.3405s/iter; left time: 9579.0450s
	iters: 900, epoch: 4 | loss: 0.3073128
	speed: 1.3416s/iter; left time: 9452.7336s
	iters: 1000, epoch: 4 | loss: 0.2473034
	speed: 1.2885s/iter; left time: 8949.5884s
	iters: 1100, epoch: 4 | loss: 0.2556080
	speed: 1.0856s/iter; left time: 7432.0266s
Epoch: 4 cost time: 1466.5688090324402
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 162, in train
    vali_loss = self.vali(vali_data, vali_loader, criterion)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 52, in vali
    batch_x = batch_x.float().to(self.device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Running Solar with seq_len=384, pred_len=192
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_384_192       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_384_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36217
val 5065
test 10321
	iters: 100, epoch: 1 | loss: 1.4807354
	speed: 1.3683s/iter; left time: 15353.7417s
	iters: 200, epoch: 1 | loss: 1.6422063
	speed: 1.3515s/iter; left time: 15029.8746s
	iters: 300, epoch: 1 | loss: 1.2651043
	speed: 1.3502s/iter; left time: 14880.3998s
	iters: 400, epoch: 1 | loss: 1.0251760
	speed: 1.3488s/iter; left time: 14730.0054s
	iters: 500, epoch: 1 | loss: 1.3122967
	speed: 1.3509s/iter; left time: 14618.3570s
	iters: 600, epoch: 1 | loss: 1.4884430
	speed: 1.3519s/iter; left time: 14493.5885s
	iters: 700, epoch: 1 | loss: 1.0362858
	speed: 1.3473s/iter; left time: 14310.0038s
	iters: 800, epoch: 1 | loss: 0.7980165
	speed: 1.3501s/iter; left time: 14203.9156s
	iters: 900, epoch: 1 | loss: 0.8548465
	speed: 1.2650s/iter; left time: 13182.4373s
	iters: 1000, epoch: 1 | loss: 0.6624599
	speed: 1.1075s/iter; left time: 11430.6761s
	iters: 1100, epoch: 1 | loss: 0.5719624
	speed: 1.0788s/iter; left time: 11026.5042s
Epoch: 1 cost time: 1460.8996686935425
Epoch: 1, Steps: 1132 | Train Loss: 1.2588275 Vali Loss: 67.7238007 Test Loss: 50.5211868
Validation loss decreased (inf --> 67.723801).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.4436042
	speed: 2.3070s/iter; left time: 23275.5796s
	iters: 200, epoch: 2 | loss: 0.3690201
	speed: 0.9680s/iter; left time: 9669.4732s
	iters: 300, epoch: 2 | loss: 0.3989453
	speed: 0.9692s/iter; left time: 9583.9912s
	iters: 400, epoch: 2 | loss: 0.3618174
	speed: 0.9625s/iter; left time: 9422.1120s
	iters: 500, epoch: 2 | loss: 0.4174176
	speed: 0.9664s/iter; left time: 9363.2108s
	iters: 600, epoch: 2 | loss: 0.3226004
	speed: 0.9729s/iter; left time: 9329.4260s
	iters: 700, epoch: 2 | loss: 0.3347863
	speed: 0.9678s/iter; left time: 9183.3159s
	iters: 800, epoch: 2 | loss: 0.3102477
	speed: 0.9697s/iter; left time: 9104.9120s
	iters: 900, epoch: 2 | loss: 0.3407417
	speed: 0.9750s/iter; left time: 9057.0131s
	iters: 1000, epoch: 2 | loss: 0.3233046
	speed: 0.9839s/iter; left time: 9041.0560s
	iters: 1100, epoch: 2 | loss: 0.3507412
	speed: 0.9631s/iter; left time: 8753.2915s
Epoch: 2 cost time: 1098.460900068283
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 162, in train
    vali_loss = self.vali(vali_data, vali_loader, criterion)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 52, in vali
    batch_x = batch_x.float().to(self.device)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Running Solar with seq_len=384, pred_len=336
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_384_336       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_384_336_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36073
val 4921
test 10177
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.64 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.14 GiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2056641 has 16.51 GiB memory in use. Of the allocated memory 15.90 GiB is allocated by PyTorch, and 114.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=512, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_512_96        Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_512_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36185
val 5161
test 10417
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 70, in forward
    V = torch.einsum("bhls,bshd->blhd", A, values)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/functional.py", line 386, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 558.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 468.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2059223 has 17.19 GiB memory in use. Of the allocated memory 16.63 GiB is allocated by PyTorch, and 72.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=512, pred_len=192
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_512_192       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_512_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 36089
val 5065
test 10321
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 70, in forward
    V = torch.einsum("bhls,bshd->blhd", A, values)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/functional.py", line 386, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 558.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 430.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2062126 has 17.22 GiB memory in use. Of the allocated memory 16.64 GiB is allocated by PyTorch, and 94.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=512, pred_len=336
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_512_336       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_512_336_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35945
val 4921
test 10177
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 70, in forward
    V = torch.einsum("bhls,bshd->blhd", A, values)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/functional.py", line 386, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 558.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 450.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2064036 has 17.21 GiB memory in use. Of the allocated memory 16.66 GiB is allocated by PyTorch, and 52.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=736, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_736_96        Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_736_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35961
val 5161
test 10417
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.11 GiB. GPU 0 has a total capacity of 79.15 GiB of which 728.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2066691 has 16.93 GiB memory in use. Of the allocated memory 16.32 GiB is allocated by PyTorch, and 126.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=736, pred_len=192
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_736_192       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_736_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35865
val 5065
test 10321
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.11 GiB. GPU 0 has a total capacity of 79.15 GiB of which 708.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2069345 has 16.95 GiB memory in use. Of the allocated memory 16.34 GiB is allocated by PyTorch, and 126.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=736, pred_len=336
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_736_336       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_736_336_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35721
val 4921
test 10177
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.11 GiB. GPU 0 has a total capacity of 79.15 GiB of which 688.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2072218 has 16.97 GiB memory in use. Of the allocated memory 16.37 GiB is allocated by PyTorch, and 115.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=736, pred_len=720
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_736_720       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_736_720_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35337
val 4537
test 9793
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 48, in forward
    y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.11 GiB. GPU 0 has a total capacity of 79.15 GiB of which 588.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2074371 has 17.07 GiB memory in use. Of the allocated memory 16.45 GiB is allocated by PyTorch, and 133.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=1024, pred_len=96
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_1024_96       Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_1024_96_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35673
val 5161
test 10417
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 75, in forward
    return V.contiguous(), None
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 79.15 GiB of which 910.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2076852 has 16.76 GiB memory in use. Of the allocated memory 16.17 GiB is allocated by PyTorch, and 100.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=1024, pred_len=192
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_1024_192      Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_1024_192_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35577
val 5065
test 10321
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 75, in forward
    return V.contiguous(), None
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 79.15 GiB of which 790.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2079504 has 16.87 GiB memory in use. Of the allocated memory 16.20 GiB is allocated by PyTorch, and 193.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=1024, pred_len=336
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_1024_336      Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_1024_336_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35433
val 4921
test 10177
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 75, in forward
    return V.contiguous(), None
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 79.15 GiB of which 830.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2082386 has 16.83 GiB memory in use. Of the allocated memory 16.24 GiB is allocated by PyTorch, and 112.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running Solar with seq_len=1024, pred_len=720
Using GPU
Args in experiment:
[1mBasic Config[0m
  Task Name:          AGPT_loss           Is Training:        1                   
  Model ID:           solar_1024_720      Model:              AGPT_loss_G         

[1mData Loader[0m
  Data:               Solar               Root Path:          ./dataset/Solar/    
  Data Path:          solar_AL.txt        Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Num Kernels:        6                   
  Enc In:             137                 Dec In:             137                 
  C Out:              137                 d model:            512                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               2048                
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        10                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : AGPT_loss_solar_1024_720_AGPT_loss_G_2048_fixedFalse_0.001_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 35049
val 4537
test 9793
Traceback (most recent call last):
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/run.py", line 138, in <module>
    exp.train(setting)
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/exp/exp_AGPT.py", line 131, in train
    outputs, aux_loss = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 226, in forward
    dec_out, budget_loss = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/models/AGPT_loss_G.py", line 202, in forecast
    enc_out, attns = self.encoder(enc_out)
                     ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
                  ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/pfs/zitao_team/kuiyeding/AGPT/layers/SelfAttention_Family.py", line 75, in forward
    return V.contiguous(), None
           ^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.08 GiB. GPU 0 has a total capacity of 79.15 GiB of which 728.12 MiB is free. Process 1960153 has 61.49 GiB memory in use. Process 2085308 has 16.93 GiB memory in use. Of the allocated memory 16.34 GiB is allocated by PyTorch, and 105.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
